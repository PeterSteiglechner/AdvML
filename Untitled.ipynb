{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD2434_Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS:\n",
    "\n",
    "k - number of topics\n",
    "N - number of words in a document (different for each document)\n",
    "M - number of documents in a corpus\n",
    "\n",
    "Model parameters:\n",
    "z_n - [k] dimension vector; topic distribution for word n \n",
    "Theta - [k] dimension vector; mixture weights\n",
    "alpha - [k] dimension vector; prior probability for theta (mixture weights) (alpha > 0)\n",
    "beta - [k x V] dimension matrix; beta_ij = p(w^j = 1 | z^i = 1) \n",
    "                                 probability for a specific word j given a specific topic i\n",
    "D - list of [V x N] dimension matrices, that is M long = [\\mathbf{w}_1, ... \\mathbf{w}_M];\n",
    "                                 where \\mathbf{w} = [w_1,...,w_N] is [V x N] (one document consisting of N words) \n",
    "\n",
    "\n",
    "Variational parameters:\n",
    "Gamma - [k] dimension vector; determines Theta in the Variational Model\n",
    "Phi = phi_1 .. phi_N - [N x k] dimension matrix; determines the probability distribution \n",
    "                                 for topics z of words in the Variational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "from scipy import special, misc\n",
    "from scipy.special import digamma, gammaln, polygamma\n",
    "import pandas as pd\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Document Corpus\n",
    "Download the data from \n",
    "https://github.com/Blei-Lab/lda-c/blob/master/example/ap.tgz\n",
    "\n",
    "We can directly load the file \"ap.dat\" which contains:\n",
    "\n",
    "1 line = 1 document,\n",
    "\n",
    "[number of different words in doc] [word index (where the one is in w_n]:[how often it occurs in the doc] [word index 2]:[occurences 2] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = np.genfromtxt('ap/vocab.txt',  dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_data(corpus_file, vocabulary_file, stopwords_file):\n",
    "    \"\"\"\n",
    "    Reads the corpus from the .txt file into a list of lists;\n",
    "    a list for each document which contains a list of all the \n",
    "    words as strings.\n",
    "    Input parameters:\n",
    "    corpus_file - path to the corpus file\n",
    "    vocabulary_file - path to the vocabulary file\n",
    "    stopwords_file - path to the stopwords file\n",
    "    \"\"\"\n",
    "    vocabulary = np.genfromtxt(vocabulary_file,  dtype='str')\n",
    "    special_chars = '1234567890~!@#Â£$%^&*()_+,./<>?\\|\"]}\\'[{`-'\n",
    "    corpus = []\n",
    "    \n",
    "    # read in stopwords from file into a list\n",
    "    stopwords = [] \n",
    "    with open(stopwords_file, 'r') as file:\n",
    "        stop_words = file.read().replace(',', ' ')\n",
    "        for word in stop_words.split():\n",
    "            stopwords.append(word) \n",
    "    \n",
    "    with open(corpus_file, 'r') as text:\n",
    "        doc = ''\n",
    "        new = False\n",
    "        for line in text:\n",
    "            if new: # reached a new document\n",
    "                if line.strip() != '</TEXT>': # until we reach the new doc\n",
    "                    for char in special_chars: # remove punctuation etc,\n",
    "                        line = line.replace(char, '') \n",
    "                    doc += line\n",
    "                else: # we've reached a new doc again\n",
    "                    doc = doc.lower() # all words lowercase\n",
    "                    words = np.array(doc.split())\n",
    "                    # PETER EDIT: next two lines\n",
    "                    doc = [word for word in words if (  (word not in stopwords) and (word in vocabulary)  )]\n",
    "                    corpus.append(doc)\n",
    "                    doc = ''\n",
    "            elif line.strip() == '<TEXT>': new = True\n",
    "\n",
    "    \n",
    "    return corpus, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocabulary = clean_up_data('ap/ap.txt', 'ap/vocab.txt', 'ap/stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(documents, vocabulary, k):\n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "    \n",
    "    # Initialize alpha \n",
    "    # alpha = np.ones([M,k]) * 50/k # for every document, for every topic\n",
    "    alpha = np.ones(k)*50/k\n",
    "    eta = 5/k\n",
    "    \n",
    "    Lambda = np.random.rand(k,V) * 0.5 + 0.5\n",
    "    \n",
    "    # Initialize beta\n",
    "    beta = np.zeros([k,V]) # for every topic, for every word in the vocabulary\n",
    "    for i in range(k):\n",
    "        beta[i] = np.random.uniform(0, 1, V)\n",
    "        beta[i] = beta[i] / np.sum(beta[i])\n",
    "    \n",
    "    # Initialize phi and gamma\n",
    "    phi = []\n",
    "    gamma = np.zeros([M,k]) # for every document, for every topic\n",
    "    for m in range(M):\n",
    "        doc = np.array(documents[m])\n",
    "        N = len(doc)\n",
    "        phi.append(np.ones([N,k]) * 1/float(k)) # uniform over topics\n",
    "        \n",
    "        for i in range(k):\n",
    "            gamma[m][i] = alpha[i] + N/float(k)\n",
    "        #m += 1 # WHYYYYYYY?\n",
    "        \n",
    "    return alpha, eta, beta, gamma, phi, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lower_bound_likelihood(phi, gamma, alpha, beta, document, vocabulary,k):\n",
    "    '''\n",
    "    This calculates the lower bound of L(gamma, phi, alpha, beta)\n",
    "    Ie. equation 15 in the paper in Appendix 3.\n",
    "    '''\n",
    "   \n",
    "    N, k = phi.shape\n",
    "    k, V = beta.shape\n",
    "    \n",
    "    loggamma_sum = lambda x: scipy.special.gammaln(np.sum(x))\n",
    "    loggamma_x_i = lambda x, i: np.log(scipy.special.gamma(x[i]))\n",
    "    E_log_thetai_givenGamma = lambda i:  (psi(gamma[i]) - psi(np.sum(gamma))) \n",
    "\n",
    "    term0 = loggamma_sum(alpha) - loggamma_sum(gamma)\n",
    "    term_kSum=0\n",
    "    for i in range(k):\n",
    "        E = E_log_thetai_givenGamma(i)\n",
    "        term_kSum += -loggamma_x_i(alpha,i) + (alpha[i]-1) * E\n",
    "        term_kSum += gammaln(gamma[i]) - (gamma[i] - 1) * E\n",
    "\n",
    "        term_knSum = 0\n",
    "        term_knvSum = 0\n",
    "        for n in range(N):\n",
    "            if phi[n,i] == 0:\n",
    "                print(\"Error: Phi[\",n,i,\"] == 0\")\n",
    "            term_knSum += phi[n,i] * E_log_thetai_givenGamma(i)\n",
    "            term_knSum += - phi[n,i] * np.log(phi[n,i])\n",
    "            \n",
    "            v = np.where(vocabulary == document[n])[0][0] # here w_n is not a vector\n",
    "            if beta[i,v] <= 0:\n",
    "                print(\"Error: beta[\"+i,v,\"]<=0\")\n",
    "            #L+= phi[n,i] * np.log(beta[i,v]) \n",
    "            term_knvSum += phi[n,i] * np.log(beta[i,v]) \n",
    "\n",
    "    #print(term0,term_knSum, term_kSum)\n",
    "    L_terms = term0 + term_knSum + term_kSum + term_knvSum\n",
    "    \n",
    "    return L_terms\n",
    "    \n",
    "\n",
    "\n",
    "def psi(gamma_i):\n",
    "    # this is the first derivative (via Taylor approximation) of the log \\Gamma function\n",
    "    # according to Wikipedia this is the \"digamma\" function\n",
    "    return scipy.special.digamma(gamma_i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeDocumentLikelihood(wd, alpha, eta, gamma, digamma_gamma, phi, digamma_lambda, d):\n",
    "\n",
    "    V, K = digamma_lambda.shape\n",
    "    N = len(wd)\n",
    "    \n",
    "    E_theta_alpha = gammaln(alpha*K) - K * gammaln(alpha) \\\n",
    "                        + (alpha-1) * np.sum(digamma_gamma)\n",
    "    \n",
    "    E_z_theta = np.dot(np.sum(phi[d][:N,:], axis = 0), digamma_gamma)\n",
    "    \n",
    "    E_w_z_beta = np.sum(digamma_lambda[d,:] * phi[d][:N,:])\n",
    "    \n",
    "    E_theta_gamma = gammaln(np.sum(gamma[d,:])) - np.sum(gammaln(gamma[d,:])) \\\n",
    "                    + np.dot(gamma[d,:] - 1, digamma_gamma)\n",
    "\n",
    "    E_z_phi = np.sum(phi[d][:N,:] * np.log(phi[d][:N,:]))\n",
    "    \n",
    "\n",
    "    Likelihood = E_theta_alpha + E_z_theta + E_w_z_beta - E_theta_gamma - E_z_phi\n",
    "    return(Likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihood_smoothed(documents, alpha, eta, gamma, phi, Lambda):\n",
    "    \n",
    "    K, V = Lambda.shape\n",
    "    D = len(documents)\n",
    "    \n",
    "    digamma_lambda = digamma(Lambda.T) - digamma(np.sum(Lambda, axis = 1))\n",
    "    likelihood = np.zeros([D,1])\n",
    "    \n",
    "    for d in range(D):\n",
    "        digamma_gamma = digamma(gamma[d,:]) - digamma(np.sum(gamma[d,:]))\n",
    "\n",
    "        N = len(documents[d])\n",
    "        E_theta_alpha = gammaln(alpha*K) - K * gammaln(alpha) \\\n",
    "                            + (alpha-1) * np.sum(digamma_gamma)\n",
    "        E_z_theta = np.dot(np.sum(phi[d][:N,:], axis = 0), digamma_gamma)\n",
    "        E_w_z_beta = np.sum(digamma_lambda[d,:] * phi[d][:N,:])\n",
    "        E_theta_gamma = gammaln(np.sum(gamma[d,:])) - np.sum(gammaln(gamma[d,:])) \\\n",
    "                        + np.dot(gamma[d,:] - 1, digamma_gamma)\n",
    "        E_z_phi = np.sum(phi[d][:N,:] * np.log(phi[d][:N,:]))\n",
    "        \n",
    "        likelihood = E_theta_alpha + E_z_theta + E_w_z_beta - E_theta_gamma - E_z_phi\n",
    "\n",
    "        E_beta_eta = K * (gammaln(eta * V) - V * gammaln(eta)) + (eta - 1) * np.sum(digamma_lambda)\n",
    "        E_beta_lambda = np.sum(gammaln(np.sum(Lambda, axis = 1)) - np.sum(gammaln(Lambda), axis = 1)[np.newaxis,:]) \\\n",
    "                        + np.sum((Lambda - 1) * digamma_lambda.T)\n",
    "    \n",
    "    likelihood = np.sum(likelihood) + E_beta_eta - E_beta_lambda\n",
    "    \n",
    "    return(likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def copied(Phi, gamma, alpha, Beta, document, vocabulary, k):\n",
    "#    likelihood = 0.0\n",
    "#    V = len(vocabulary)\n",
    "#    words = np.array(document)\n",
    "#    N = len(words)\n",
    "#    print(N,V,k)\n",
    "#    \n",
    "#    alpha_sum = 0.0\n",
    "#    phi_gamma_sum = 0.0\n",
    "#    phi_logbeta_sum = 0.0\n",
    "#    entropy_sum = 0.0\n",
    "#    gamma_sum = 0.0\n",
    "#    \n",
    "#    alpha_sum += gammaln(np.sum(alpha))  \n",
    "#    gamma_sum -= gammaln(np.sum(gamma)) \n",
    "#    \n",
    "#    for i in range(k):\n",
    "#        alpha_sum += -gammaln(alpha[i]) + \\\n",
    "#                (alpha[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma)))\n",
    "#        \n",
    "#        for n in range(N):\n",
    "#            if Phi[n,i] > 0:\n",
    "#                w_indicator = np.sum(np.in1d(vocabulary, words[n]))   \n",
    "#                phi_gamma_sum += Phi[n,i] * (digamma(gamma[i]) - digamma(np.sum(gamma[:])))\n",
    "#                entropy_sum += Phi[n,i] * np.log(Phi[n,i])\n",
    "#                for j in range(V):\n",
    "#                    if Beta[i,j] > 0:\n",
    "#                        phi_logbeta_sum += Phi[n,i] * w_indicator * np.log(Beta[i,j])\n",
    "#                        #phi_logbeta_sum+=0\n",
    "#        \n",
    "#        gamma_sum += gammaln(gamma[i]) - \\\n",
    "#                    (gamma[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma[:])))\n",
    "#        \n",
    "#        print(\"i: \")\n",
    "#        print(-gammaln(alpha[i]) + \\\n",
    "#                (alpha[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma))))\n",
    "#        print(gammaln(gamma[i]) - \\\n",
    "#                    (gamma[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma[:]))))\n",
    "#        \n",
    "#    \n",
    "#    likelihood += (alpha_sum + phi_gamma_sum + phi_logbeta_sum - gamma_sum - entropy_sum) \n",
    "#    \n",
    "#    print(alpha_sum, gamma_sum, phi_gamma_sum,entropy_sum)\n",
    "#    print(\"L=\",alpha_sum+phi_gamma_sum-gamma_sum-entropy_sum)\n",
    "#\n",
    "#    print(phi_logbeta_sum)\n",
    "#    return likelihood#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_phi_gamma(k, phi, gamma, alpha, beta, document, vocabulary, tol=1e-5, MAX_STEPS = 100):\n",
    "    \n",
    "    likelihood = 0.0\n",
    "    iterations = 0\n",
    "    converged = False\n",
    "    \n",
    "    words = np.array(document)\n",
    "    N = len(words)\n",
    "\n",
    "    while (not converged) and (iterations < MAX_STEPS):\n",
    "        iterations += 1\n",
    "            \n",
    "        phi_old = phi\n",
    "        phi = np.zeros([N,k])\n",
    "        gamma_old = gamma\n",
    "            \n",
    "        for n in range(N):\n",
    "            word = words[n]\n",
    "            if len(np.where(vocabulary == word)[0]) > 0: # word exists in vocabulary\n",
    "                for i in range(k):                \n",
    "                    beta_ = beta[i, np.where(vocabulary == word)]\n",
    "                    phi[n, i] = beta_[0][0] * np.exp(digamma(gamma[i]) - digamma(np.sum(gamma)))\n",
    "                phi[n,:] = phi[n,:] / np.sum(phi[n,:])   \n",
    "        gamma = alpha + np.sum(phi, axis=0)    \n",
    "            \n",
    "\n",
    "        # Convergence ctierion: did phi and gamma change significantly?\n",
    "        if (np.linalg.norm(phi - phi_old) < tol) and (np.linalg.norm(gamma - gamma_old) < tol):              \n",
    "            print(str(iterations) + ' iterations to converge.')\n",
    "                \n",
    "            likelihood += compute_lower_bound_likelihood(phi, gamma, alpha, beta, document, vocabulary, k)\n",
    "            converged = True\n",
    "    \n",
    "    return phi, gamma, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_phi_gamma_smoothed(documents, alpha, eta, gamma, phi, Lambda, tol = 1e-4, MAX_STEPS = 100):\n",
    "    K, V = Lambda.shape\n",
    "    D = len(documents)\n",
    "    \n",
    "    digamma_lambda = digamma(Lambda.T) - digamma(np.sum(Lambda, axis = 1))\n",
    "    for d in range(D):\n",
    "        likelihood = -1e9\n",
    "        converged = False\n",
    "        iterations = 0\n",
    "        \n",
    "        while(not converged):\n",
    "\n",
    "            iterations += 1\n",
    "            wd = documents[d]\n",
    "            \n",
    "            digamma_gamma = digamma(gamma[d,:]) - digamma(np.sum(gamma[d,:]))\n",
    "            \n",
    "            N = len(documents[d])\n",
    "            \n",
    "            phi[d][:N,:] = digamma_gamma + digamma_lambda[d,:]\n",
    "            phi[d][:N,:] = np.exp(phi[d][:N,:] - misc.logsumexp(phi[d][:N,:], axis = 1)[:,np.newaxis])\n",
    "            \n",
    "            gamma[d,:] = alpha + np.sum(phi[d][:N,:], axis = 0)\n",
    "            \n",
    "            newLikelihood = ComputeDocumentLikelihood(wd, alpha, eta, gamma, digamma_gamma, phi, digamma_lambda, d)\n",
    "            dlikelihood = abs((newLikelihood - likelihood)/likelihood)\n",
    "            likelihood = newLikelihood\n",
    "            \n",
    "            if(dlikelihood < tol).any():\n",
    "                converged = True\n",
    "    \n",
    "    #print gamma\n",
    "    return(phi, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lambda(phi, eta, documents, vocabulary, k):\n",
    "    \n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "    \n",
    "    Lambda = np.ones([k, V]) * eta\n",
    "    for m, doc in enumerate(documents):\n",
    "        words = np.array(doc)\n",
    "        phi_m = phi[m]\n",
    "        for i in range(k):\n",
    "            phi_ = phi_m[:,i]\n",
    "            for j in range(V):\n",
    "                word = vocabulary[j]\n",
    "                indicator = np.in1d(words, word)\n",
    "                indicator.astype(int)  \n",
    "                Lambda[i][j] += np.dot(indicator, phi_)\n",
    "                    \n",
    "    return Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_beta(phi, documents, vocabulary, k):\n",
    "    \n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "    \n",
    "    beta = np.zeros([k, V])\n",
    "    for m, doc in enumerate(documents):\n",
    "        words = np.array(doc)\n",
    "        phi_m = phi[m]\n",
    "        for i in range(k):\n",
    "            phi_ = phi_m[:,i]\n",
    "            for j in range(V):\n",
    "                word = vocabulary[j]\n",
    "                indicator = np.in1d(words, word)\n",
    "                indicator.astype(int) \n",
    "                beta[i][j] += np.dot(indicator, phi_)\n",
    "    beta = np.transpose(np.transpose(beta) / np.sum(beta, axis=1))\n",
    "\n",
    "    return beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha(alpha, gamma, k, M, max_iter=50, tol=1e-4):\n",
    "    \n",
    "    # Maria B version\n",
    "    temp = 0\n",
    "    for d in range(M):\n",
    "        temp_1 = np.sum(special.polygamma(0, gamma[d])) - np.sum(special.polygamma(0, np.sum(gamma, axis=1)))\n",
    "    \n",
    "    gradient = M * (k * special.polygamma(1, alpha) - special.polygamma(1, k*alpha))\n",
    "    gradient = gradient + temp\n",
    "\n",
    "    hessian = M * k * (k * special.polygamma(2, k*alpha) - special.polygamma(2, alpha))\n",
    "\n",
    "    temp = gradient / (hessian * alpha + gradient + tol)\n",
    "    if (alpha == 0).any():\n",
    "        alpha += 0.005\n",
    "\n",
    "    log_alpha = np.log(alpha) - temp\n",
    "    alpha = np.exp(log_alpha)    \n",
    "        \n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_eta(eta, gamma, k, V, M, max_iter=50, tol=1e-4):\n",
    "    \n",
    "\n",
    "    temp = 0\n",
    "    for d in range(M):\n",
    "        temp_1 = np.sum(special.polygamma(0, gamma[d])) - np.sum(special.polygamma(0, np.sum(gamma, axis=1)))\n",
    "    \n",
    "    gradient = V * (k * special.polygamma(1, eta) - special.polygamma(1, k*eta))\n",
    "    gradient = gradient + temp\n",
    "\n",
    "    hessian = V * k * (k * special.polygamma(2, k*eta) - special.polygamma(2, eta))\n",
    "\n",
    "    temp = gradient / (hessian * eta + gradient + tol)\n",
    "    if (eta == 0):\n",
    "        eta += 0.005\n",
    "\n",
    "    log_eta = np.log(eta) - temp\n",
    "    eta = np.exp(log_eta)    \n",
    "        \n",
    "    return eta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(phi, gamma, alpha, beta, documents, vocabulary, k):\n",
    "\n",
    "    for d, doc in enumerate(documents):\n",
    "        phi[d], gamma[d], likelihood = update_phi_gamma(k, phi[d], gamma[d], alpha, beta, doc, vocabulary)\n",
    "                \n",
    "    return phi, gamma, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step_smoothed(documents, alpha, eta, gamma, phi, Lambda, Tol = 1e-5, MAX_STEPS = 100):\n",
    "\n",
    "    K, V = Lambda.shape\n",
    "    D = len(documents)\n",
    "    \n",
    "    iterations = 0\n",
    "    likelihood = -1e9\n",
    "    converged = False\n",
    "    while(not(converged)):\n",
    "        \n",
    "        iterations += 1\n",
    "        phi, gamma = update_phi_gamma_smoothed(documents, alpha, eta, gamma, phi, Lambda)\n",
    "        Lambda = update_lambda(phi, eta, documents, vocabulary, k)\n",
    "\n",
    "\n",
    "        newLikelihood = compute_likelihood_smoothed(documents, alpha, eta, gamma, phi, Lambda)\n",
    "        dlikelihood = abs((newLikelihood - likelihood)/likelihood)\n",
    "        likelihood = newLikelihood\n",
    "\n",
    "        if(dlikelihood < Tol).any():\n",
    "            print('E-step converged after %d iterations' %iterations)\n",
    "            converged = True\n",
    "    return(phi, gamma, Lambda, likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(phi, gamma, alpha, documents, vocabulary, k):\n",
    "    print('M-step')\n",
    "    beta = update_beta(phi, documents, vocabulary, k)\n",
    "    alpha = update_alpha(alpha, gamma, k, M)\n",
    "    \n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step_smoothed(phi, gamma, alpha, eta, documents, vocabulary, k):\n",
    "    print('M-step')\n",
    "    \n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    alpha = update_alpha(alpha, gamma, k, M)\n",
    "    eta = update_eta(eta, gamma, k, V, M)\n",
    "    print(\"NEW DEBU\")\n",
    "    print(eta)\n",
    "    \n",
    "    return alpha, eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_EM(phi_init, gamma_init, alpha_init, beta_init, documents, vocabulary, k, tol=1e-5):\n",
    "    print('Variational EM')\n",
    "    \n",
    "    M = len(documents)\n",
    "    \n",
    "    likelihood = 0\n",
    "    likelihood_old = 0.000004\n",
    "    \n",
    "    iteration = 1 # Initialization step is the first step\n",
    "    \n",
    "    phi = phi_init\n",
    "    gamma = gamma_init\n",
    "    alpha = alpha_init\n",
    "    beta = beta_init\n",
    "    \n",
    "    converged = False\n",
    "    \n",
    "    while (not converged):\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Update parameters \n",
    "        if likelihood == 0:\n",
    "            print(\"Likelihood==0\")\n",
    "            likelihood_old = 0.005\n",
    "        else:\n",
    "            likelihood_old = likelihood\n",
    "        phi_old = phi \n",
    "        gamma_old = gamma \n",
    "        alpha_old = alpha\n",
    "        beta_old = beta\n",
    "    \n",
    "        phi, gamma, likelihood = \\\n",
    "            E_step(phi_old, gamma_old, alpha_old, beta_old, documents, vocabulary, k)\n",
    "        alpha, Beta = \\\n",
    "            M_step(phi, gamma, alpha_old, documents, vocabulary, k)\n",
    "                \n",
    "        if iteration > 15:\n",
    "            break\n",
    "        \n",
    "        # check convergence\n",
    "        if (np.abs((likelihood-likelihood_old)/likelihood_old) > tol):\n",
    "            if (iteration > 2):\n",
    "                converged = True\n",
    "        \n",
    "    return phi, gamma, alpha, Beta, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_EM_smoothed(phi_init, gamma_init, alpha_init, beta_init, Lambda_init, eta_init, documents, vocabulary, k, tol=1e-5):\n",
    "    print('Variational EM')\n",
    "    \n",
    "    M = len(documents)\n",
    "    \n",
    "    likelihood = 0\n",
    "    likelihood_old = 0.000004\n",
    "    \n",
    "    iteration = 1 # Initialization step is the first step\n",
    "    \n",
    "    phi = phi_init\n",
    "    gamma = gamma_init\n",
    "    alpha = alpha_init\n",
    "    beta = beta_init\n",
    "    Lambda = Lambda_init\n",
    "    eta = eta_init\n",
    "    \n",
    "    converged = False\n",
    "    \n",
    "    while (not converged):\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Update parameters \n",
    "        if likelihood == 0:\n",
    "            print(\"Likelihood==0\")\n",
    "            likelihood_old = 0.005\n",
    "        else:\n",
    "            likelihood_old = likelihood\n",
    "        phi_old = phi \n",
    "        gamma_old = gamma \n",
    "        alpha_old = alpha\n",
    "        beta_old = beta\n",
    "        Lambda_old = Lambda\n",
    "        eta_old = eta\n",
    "        \n",
    "    \n",
    "        phi, gamma, Lambda, likelihood = \\\n",
    "            E_step_smoothed(documents, alpha_old, eta_old, gamma_old, phi_old, Lambda_old)\n",
    "        alpha, eta = \\\n",
    "            M_step_smoothed(phi, gamma, alpha, eta, documents, vocabulary, k)\n",
    "                \n",
    "        if iteration > 15:\n",
    "            break\n",
    "        \n",
    "        # check convergence\n",
    "        if (np.abs((likelihood-likelihood_old)/likelihood_old) > tol):\n",
    "            if (iteration > 2):\n",
    "                converged = True\n",
    "        \n",
    "    return phi, gamma, Lambda, alpha, eta, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN: LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "corpus_reduced = corpus[:3]\n",
    "M = len(corpus_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_init, eta_init, beta_init, gamma_init, phi_init, Lambda_init = initialize_parameters(corpus_reduced, vocabulary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational EM\n",
      "Likelihood==0\n",
      "44 iterations to converge.\n",
      "47 iterations to converge.\n",
      "42 iterations to converge.\n",
      "M-step\n",
      "54 iterations to converge.\n",
      "56 iterations to converge.\n",
      "48 iterations to converge.\n",
      "M-step\n"
     ]
    }
   ],
   "source": [
    "phi, gamma, alpha, beta, likelihood = \\\n",
    "        variational_EM(phi_init, gamma_init, alpha_init, beta_init, corpus_reduced, vocabulary, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN: smoothed LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_init, eta_init, beta_init, gamma_init, phi_init, Lambda_init = initialize_parameters(corpus_reduced, vocabulary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational EM\n",
      "Likelihood==0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bjeli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: `logsumexp` is deprecated!\n",
      "Importing `logsumexp` from scipy.misc is deprecated in scipy 1.0.0. Use `scipy.special.logsumexp` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-step converged after 11 iterations\n",
      "M-step\n",
      "NEW DEBU\n",
      "1.0352422730586692\n",
      "E-step converged after 2 iterations\n",
      "M-step\n",
      "NEW DEBU\n",
      "0.6663281132982769\n"
     ]
    }
   ],
   "source": [
    "phi, gamma, Lambda, alpha, eta, likelihood = \\\n",
    "variational_EM_smoothed(phi_init, gamma_init, alpha_init, beta_init, Lambda_init, eta_init, corpus_reduced, vocabulary, k, tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(gamma.T)\n",
    "plt.xlabel(\"Documents\")\n",
    "plt.ylabel(\"topic 1..k\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.pcolormesh(Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(k):\n",
    "    beta_topic = beta[l,:]\n",
    "    beta_topic_top4 = np.argsort(beta_topic)[-5:]\n",
    "    plt.plot(beta_topic)\n",
    "    print([w for w in np.array(vocabulary)[beta_topic_top4][:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITE TEXT WITH COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_text(corpus_file, number_of_text):\n",
    "    fulltext_words=[]\n",
    "    fulltext_allwords=[]\n",
    "    #number_of_text=2\n",
    "    text_counter=0\n",
    "    special_chars = '1234567890~!@#Â£$%^&*()_+,./<>?\\|\"]}\\'[{`-'\n",
    "\n",
    "    with open(corpus_file, 'r') as text:\n",
    "        new=False\n",
    "        for line in text:\n",
    "            if new:\n",
    "                #print(line.strip()[0], line.strip()[:10])\n",
    "                if line.strip()[0]==\"<\":\n",
    "                    pass\n",
    "                else:\n",
    "                    #print(\"FOUND\", text)\n",
    "                    text_counter+=1\n",
    "                    if text_counter==number_of_text:\n",
    "                        #print(\" CORRECT\")\n",
    "                        new_text=line\n",
    "                        fulltext=new_text\n",
    "                        words = np.array(new_text.split())\n",
    "                        for word in words:\n",
    "                            fulltext_allwords.append(word)\n",
    "                            for char in special_chars: # remove punctuation etc,\n",
    "                                word = word.replace(char, '') \n",
    "                            fulltext_words.append(word)\n",
    "\n",
    "\n",
    "            else:\n",
    "                if line.strip() == \"<TEXT>\":\n",
    "                    new=True\n",
    "    return fulltext_words, fulltext_allwords\n",
    "\n",
    "fulltext_words, fulltext_allwords = get_full_text('ap/ap.txt', 13)\n",
    "\" \".join(fulltext_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=['blue','green', 'red']\n",
    "fulltext_colors=[]\n",
    "\n",
    "how_significant=2.5\n",
    "significance=[]\n",
    "for word in fulltext_words:\n",
    "    if word in vocabulary:\n",
    "        v = np.where(vocabulary==word)[0][0]\n",
    "        #print(v,word_beta )\n",
    "        word_beta = beta[:,v]\n",
    "        #significance.append(np.max(word_beta)/np.mean(word_beta))\n",
    "        if np.max(word_beta)>np.mean(beta):\n",
    "            if np.max(word_beta)> how_significant*np.mean(word_beta):\n",
    "                #significance =  (np.max(word_beta) / np.mean(word_beta) > 10)\n",
    "                topic = np.where(np.max(word_beta)==word_beta)[0][0]\n",
    "                color = colors[topic]\n",
    "                print(word+\"=\",str( topic)+\", \", end=\"\")\n",
    "\n",
    "            else:\n",
    "                color='k'\n",
    "        else:\n",
    "            color='k'\n",
    "    else: \n",
    "        color='k'\n",
    "    fulltext_colors.append(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://ozzmaker.com/add-colour-to-text-in-python/\n",
    "#print(\"Examples of how to use ANSI COLOR: \\033[1;37;40m White          \\033[0m 1;37;40m            \\033[0;37;40m Light Grey \\033[0m 0;37;40m               \\033[0;37;48m Black      \\033[0m 0;37;48m\")\n",
    "colors_ansi=[34, 32,31] #blue, green, red\n",
    "             \n",
    "             \n",
    "for a in range(len(fulltext_allwords)):\n",
    "    if fulltext_colors[a]=='k':\n",
    "        #IF WE WANT TO FOCUS ON THE TOPIC WORDS:\n",
    "        #print(\"\\033[0;37;48m\"+fulltext_allwords[a], end=\" \")\n",
    "        print(\"\\033[0m\"+fulltext_allwords[a], end=\" \")\n",
    "        \n",
    "        #print(fulltext_allwords[a], end=\" \")\n",
    "    else:\n",
    "        for j in range(k):\n",
    "             if fulltext_colors[a]==colors[j]:\n",
    "                print(\"\\033[0;\"+str(colors_ansi[j])+\";48m\"+fulltext_allwords[a], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words =[]\n",
    "for i in range(k):\n",
    "    topic_words.append(np.where(np.array(fulltext_colors)==colors[i]))\n",
    "    print(\"\\033[0mTOPIC: \"+str(i)+\": \"+\"\\033[0;\"+str(colors_ansi[i])+\";48m\"+ \", \".join(np.array(fulltext_words)[topic_words[i]]))\n",
    "    print(\"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REUTERS DATA (get from the external program to keep this a bit tidier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getReuters import D_reuters as corpus_reuters\n",
    "from getReuters import vocab_list as vocabulary_reuters\n",
    "vocabulary_reuters=np.array(vocabulary_reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_reuters = 2\n",
    "corpus_reuters_reduced = corpus_reuters[:10]\n",
    "M_reuters = len(corpus_reuters_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_init_reuters, beta_init_reuters, gamma_init_reuters, phi_init_reuters =\\\n",
    "    initialize_parameters(corpus_reuters_reduced, vocabulary_reuters, k_reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_reuters, gamma_reuters, alpha_reuters, Beta_reuters, likelihood_reuters = \\\n",
    "        variational_EM(phi_init_reuters, gamma_init_reuters, alpha_init_reuters, beta_init_reuters, \n",
    "                       corpus_reuters_reduced, vocabulary_reuters, k_reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Analysis for REUTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(Beta_reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_LDA_vEM():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
