{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD2434_Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS:\n",
    "\n",
    "k - number of topics\n",
    "N - number of words in a document (different for each document)\n",
    "M - number of documents in a corpus\n",
    "\n",
    "Model parameters:\n",
    "z_n - [k] dimension vector; topic distribution for word n \n",
    "Theta - [k] dimension vector; mixture weights\n",
    "alpha - [k] dimension vector; prior probability for theta (mixture weights) (alpha > 0)\n",
    "beta - [k x V] dimension matrix; beta_ij = p(w^j = 1 | z^i = 1) \n",
    "                                 probability for a specific word j given a specific topic i\n",
    "D - list of [V x N] dimension matrices, that is M long = [\\mathbf{w}_1, ... \\mathbf{w}_M];\n",
    "                                 where \\mathbf{w} = [w_1,...,w_N] is [V x N] (one document consisting of N words) \n",
    "\n",
    "\n",
    "Variational parameters:\n",
    "Gamma - [k] dimension vector; determines Theta in the Variational Model\n",
    "Phi = phi_1 .. phi_N - [N x k] dimension matrix; determines the probability distribution \n",
    "                                 for topics z of words in the Variational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "from scipy import special\n",
    "from scipy.special import digamma, gammaln, polygamma\n",
    "import pandas as pd\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Document Corpus\n",
    "Download the data from \n",
    "https://github.com/Blei-Lab/lda-c/blob/master/example/ap.tgz\n",
    "\n",
    "We can directly load the file \"ap.dat\" which contains:\n",
    "\n",
    "1 line = 1 document,\n",
    "\n",
    "[number of different words in doc] [word index (where the one is in w_n]:[how often it occurs in the doc] [word index 2]:[occurences 2] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = np.genfromtxt('ap/vocab.txt',  dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_data(corpus_file, vocabulary_file, stopwords_file):\n",
    "    \"\"\"\n",
    "    Reads the corpus from the .txt file into a list of lists;\n",
    "    a list for each document which contains a list of all the \n",
    "    words as strings.\n",
    "    Input parameters:\n",
    "    corpus_file - path to the corpus file\n",
    "    vocabulary_file - path to the vocabulary file\n",
    "    stopwords_file - path to the stopwords file\n",
    "    \"\"\"\n",
    "    vocabulary = np.genfromtxt(vocabulary_file,  dtype='str')\n",
    "    special_chars = '1234567890~!@#£$%^&*()_+,./<>?\\|\"]}\\'[{`-'\n",
    "    corpus = []\n",
    "    \n",
    "    # read in stopwords from file into a list\n",
    "    stopwords = [] \n",
    "    with open(stopwords_file, 'r') as file:\n",
    "        stop_words = file.read().replace(',', ' ')\n",
    "        for word in stop_words.split():\n",
    "            stopwords.append(word) \n",
    "    \n",
    "    with open(corpus_file, 'r') as text:\n",
    "        doc = ''\n",
    "        new = False\n",
    "        for line in text:\n",
    "            if new: # reached a new document\n",
    "                if line.strip() != '</TEXT>': # until we reach the new doc\n",
    "                    for char in special_chars: # remove punctuation etc,\n",
    "                        line = line.replace(char, '') \n",
    "                    doc += line\n",
    "                else: # we've reached a new doc again\n",
    "                    doc = doc.lower() # all words lowercase\n",
    "                    words = np.array(doc.split())\n",
    "                    # PETER EDIT: next two lines\n",
    "                    doc = [word for word in words if (  (word not in stopwords) and (word in vocabulary)  )]\n",
    "                    corpus.append(doc)\n",
    "                    doc = ''\n",
    "            elif line.strip() == '<TEXT>': new = True\n",
    "\n",
    "    \n",
    "    return corpus, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocabulary = clean_up_data('ap/ap.txt', 'ap/vocab.txt', 'ap/stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(documents, vocabulary, k):\n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "    \n",
    "    # Initialize alpha \n",
    "    # alpha = np.ones([M,k]) * 50/k # for every document, for every topic\n",
    "    alpha = np.ones(k)*50/k\n",
    "    # Initialize beta\n",
    "    beta = np.zeros([k,V]) # for every topic, for every word in the vocabulary\n",
    "    for i in range(k):\n",
    "        beta[i] = np.random.uniform(0, 1, V)\n",
    "        beta[i] = beta[i] / np.sum(beta[i])\n",
    "    \n",
    "    # Initialize phi and gamma\n",
    "    phi = []\n",
    "    gamma = np.zeros([M,k]) # for every document, for every topic\n",
    "    for m in range(M):\n",
    "        doc = np.array(documents[m])\n",
    "        N = len(doc)\n",
    "        phi.append(np.ones([N,k]) * 1/float(k)) # uniform over topics\n",
    "        \n",
    "        for i in range(k):\n",
    "            gamma[m][i] = alpha[i] + N/float(k)\n",
    "        #m += 1 # WHYYYYYYY?\n",
    "        \n",
    "    return alpha, beta, gamma, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lower_bound_likelihood(phi, gamma, alpha, beta, document, vocabulary,k):\n",
    "    '''\n",
    "    This calculate the lower bound of L(gamma, phi, alpha, beta)\n",
    "    Ie. equation 15 in the paper in Appendix 3.\n",
    "    '''\n",
    "   \n",
    "    N, k = phi.shape\n",
    "    k,V = beta.shape\n",
    "    \n",
    "    loggamma_sum = lambda x: scipy.special.gammaln(np.sum(x))\n",
    "    loggamma_x_i = lambda x, i: np.log(scipy.special.gamma(x[i]))\n",
    "    E_log_thetai_givenGamma = lambda i:  (psi(gamma[i]) - psi(np.sum(gamma))) \n",
    "\n",
    "    term0 = loggamma_sum(alpha) - loggamma_sum(gamma)\n",
    "    term_kSum=0\n",
    "    for i in range(k):\n",
    "        E = E_log_thetai_givenGamma(i)\n",
    "        term_kSum += -loggamma_x_i(alpha,i) + (alpha[i]-1)*E\n",
    "        term_kSum += gammaln(gamma[i]) - (gamma[i] - 1) * E\n",
    "\n",
    "        term_knSum = 0\n",
    "        term_knvSum=0\n",
    "        for n in range(N):\n",
    "            if phi[n,i]==0:\n",
    "                print(\"Error: Phi[\",n,i,\"]==0\")\n",
    "            term_knSum += phi[n,i] * E_log_thetai_givenGamma(i)\n",
    "            term_knSum += - phi[n,i] *np.log(phi[n,i])\n",
    "            \n",
    "            v = np.where(vocabulary == document[n])[0][0] # here w_n is not a vector\n",
    "            if beta[i,v]<=0:\n",
    "                print(\"Error: beta[\"+i,v,\"]<=0\")\n",
    "            #L+= phi[n,i] * np.log(beta[i,v]) \n",
    "            term_knvSum += phi[n,i] * np.log(beta[i,v]) \n",
    "\n",
    "    #print(term0,term_knSum, term_kSum)\n",
    "    L_terms = term0+term_knSum+term_kSum+term_knvSum\n",
    "    \n",
    "    return L_terms\n",
    "    \n",
    "\n",
    "\n",
    "def psi(gamma_i):\n",
    "    # this is the first derivative (via Taylor approximation) of the log \\Gamma function\n",
    "    # according to Wikipedia this is the \"digamma\" function\n",
    "    return scipy.special.digamma(gamma_i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def copied(Phi, gamma, alpha, Beta, document, vocabulary, k):\n",
    "#    likelihood = 0.0\n",
    "#    V = len(vocabulary)\n",
    "#    words = np.array(document)\n",
    "#    N = len(words)\n",
    "#    print(N,V,k)\n",
    "#    \n",
    "#    alpha_sum = 0.0\n",
    "#    phi_gamma_sum = 0.0\n",
    "#    phi_logbeta_sum = 0.0\n",
    "#    entropy_sum = 0.0\n",
    "#    gamma_sum = 0.0\n",
    "#    \n",
    "#    alpha_sum += gammaln(np.sum(alpha))  \n",
    "#    gamma_sum -= gammaln(np.sum(gamma)) \n",
    "#    \n",
    "#    for i in range(k):\n",
    "#        alpha_sum += -gammaln(alpha[i]) + \\\n",
    "#                (alpha[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma)))\n",
    "#        \n",
    "#        for n in range(N):\n",
    "#            if Phi[n,i] > 0:\n",
    "#                w_indicator = np.sum(np.in1d(vocabulary, words[n]))   \n",
    "#                phi_gamma_sum += Phi[n,i] * (digamma(gamma[i]) - digamma(np.sum(gamma[:])))\n",
    "#                entropy_sum += Phi[n,i] * np.log(Phi[n,i])\n",
    "#                for j in range(V):\n",
    "#                    if Beta[i,j] > 0:\n",
    "#                        phi_logbeta_sum += Phi[n,i] * w_indicator * np.log(Beta[i,j])\n",
    "#                        #phi_logbeta_sum+=0\n",
    "#        \n",
    "#        gamma_sum += gammaln(gamma[i]) - \\\n",
    "#                    (gamma[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma[:])))\n",
    "#        \n",
    "#        print(\"i: \")\n",
    "#        print(-gammaln(alpha[i]) + \\\n",
    "#                (alpha[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma))))\n",
    "#        print(gammaln(gamma[i]) - \\\n",
    "#                    (gamma[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma[:]))))\n",
    "#        \n",
    "#    \n",
    "#    likelihood += (alpha_sum + phi_gamma_sum + phi_logbeta_sum - gamma_sum - entropy_sum) \n",
    "#    \n",
    "#    print(alpha_sum, gamma_sum, phi_gamma_sum,entropy_sum)\n",
    "#    print(\"L=\",alpha_sum+phi_gamma_sum-gamma_sum-entropy_sum)\n",
    "#\n",
    "#    print(phi_logbeta_sum)\n",
    "#    return likelihood#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_phi_gamma(k, phi, gamma, alpha, beta, document, vocabulary, tol=1e-5, MAX_STEPS = 100):\n",
    "    \n",
    "    likelihood = 0.0\n",
    "    security_count = 0\n",
    "    converged = False\n",
    "    \n",
    "    words = np.array(document)\n",
    "    N = len(words)\n",
    "\n",
    "    while (not converged) and (security_count < MAX_STEPS):\n",
    "        security_count += 1\n",
    "            \n",
    "        phi_old = phi\n",
    "        phi = np.zeros([N,k])\n",
    "        gamma_old = gamma\n",
    "            \n",
    "        for n in range(N):\n",
    "            word = words[n]\n",
    "            if len(np.where(vocabulary == word)[0]) > 0: # word exists in vocabulary\n",
    "                for i in range(k):                \n",
    "                    beta_ = beta[i, np.where(vocabulary == word)]\n",
    "                    phi[n, i] = beta_[0][0] * np.exp(digamma(gamma[i]) - digamma(np.sum(gamma)))\n",
    "                phi[n,:] = phi[n,:] / np.sum(phi[n,:])   \n",
    "        gamma = alpha + np.sum(phi, axis=0)    \n",
    "            \n",
    "\n",
    "        # Convergence ctierion: did phi and gamma change significantly?\n",
    "        if (np.linalg.norm(phi - phi_old) < tol) and (np.linalg.norm(gamma - gamma_old) < tol):              \n",
    "            print('Document needed ' + str(security_count) + ' iterations to converge.')\n",
    "                \n",
    "            likelihood += compute_lower_bound_likelihood(phi, gamma, alpha, beta, document, vocabulary, k)\n",
    "            converged = True\n",
    "    \n",
    "    return phi, gamma, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_beta(phi, documents, vocabulary, k):\n",
    "    \n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "    \n",
    "    beta = np.zeros([k, V])\n",
    "    for m, doc in enumerate(documents):\n",
    "        words = np.array(doc)\n",
    "        phi_m = phi[m]\n",
    "        for i in range(k):\n",
    "            phi_ = phi_m[:,i]\n",
    "            for j in range(V):\n",
    "                word = vocabulary[j]\n",
    "                indicator = np.in1d(words, word)\n",
    "                indicator.astype(int) \n",
    "                beta[i][j] += np.dot(indicator, phi_)\n",
    "    beta = np.transpose(np.transpose(beta) / np.sum(beta, axis=1))\n",
    "\n",
    "    return beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lambda(phi, ni, documents, vocabulary, k):\n",
    "    \n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "    \n",
    "    Lambda = np.ones([k, V])*ni\n",
    "    for m, doc in enumerate(documents):\n",
    "        words = np.array(doc)\n",
    "        phi_m = phi[m]\n",
    "        for i in range(k):\n",
    "            phi_ = phi_m[:,i]\n",
    "            for n, j in enumerate(words):\n",
    "                Lambda[i][j] += phi_[n]\n",
    "\n",
    "\n",
    "    return Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha(alpha, gamma, k, M, max_iter=50, tol=0.0001):\n",
    "    \"\"\"This function updates alpha.\"\"\"\n",
    "    \n",
    "    # Maria B version\n",
    "    temp = 0\n",
    "    for d in range(M):\n",
    "        temp_1 = np.sum(special.polygamma(0, gamma[d])) - np.sum(special.polygamma(0, np.sum(gamma, axis=1)))\n",
    "    \n",
    "    gradient = M * (k * special.polygamma(1, alpha) - special.polygamma(1, k*alpha))\n",
    "    gradient = gradient + temp\n",
    "\n",
    "    hessian = M * k * (k * special.polygamma(2, k*alpha) - special.polygamma(2, alpha))\n",
    "\n",
    "    temp = gradient / (hessian * alpha + gradient + tol)\n",
    "    if (alpha == 0).any():\n",
    "        alpha += 0.005\n",
    "\n",
    "    log_alpha = np.log(alpha) - temp\n",
    "    alpha = np.exp(log_alpha)    \n",
    "        \n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(phi, gamma, alpha, beta, documents, vocabulary, k):\n",
    "\n",
    "    for m, doc in enumerate(documents):\n",
    "        # words = np.array(documents[m])\n",
    "        # N = len(words)\n",
    "        \n",
    "        phi[m], gamma[m], likelihood = update_phi_gamma(k, phi[m], gamma[m], alpha, beta, doc, vocabulary)\n",
    "                \n",
    "    return phi, gamma, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(phi, gamma, alpha, documents, vocabulary, k):\n",
    "    print('M-step')\n",
    "    beta = update_beta(phi, documents, vocabulary, k)\n",
    "    alpha = update_alpha(alpha, gamma, k, M)\n",
    "    \n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "corpus = corpus[:2]\n",
    "M = len(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_EM(Phi_init, gamma_init, alpha_init, Beta_init, documents, vocabulary, k):\n",
    "    print('Variational EM')\n",
    "    M = len(documents)\n",
    "    \n",
    "    likelihood = 0\n",
    "    likelihood_old = 0.000004\n",
    "    iteration = 1 # Initialization step is the first step\n",
    "    Phi = Phi_init\n",
    "    gamma = gamma_init\n",
    "    alpha = alpha_init\n",
    "    Beta = Beta_init\n",
    "    while iteration <= 2 or np.abs((likelihood-likelihood_old)/likelihood_old) > 1e-5:\n",
    "        \n",
    "        # Update parameters \n",
    "        if likelihood == 0:\n",
    "            print(\"Likelihood==0\")\n",
    "            likelihood_old = 0.005\n",
    "        else:\n",
    "            likelihood_old = likelihood\n",
    "        Phi_old = Phi \n",
    "        gamma_old = gamma \n",
    "        alpha_old = alpha\n",
    "        Beta_old = Beta\n",
    "    \n",
    "        Phi, gamma, likelihood = \\\n",
    "            E_step(Phi_old, gamma_old, alpha_old, Beta_old, documents, vocabulary, k)\n",
    "        alpha, Beta = \\\n",
    "            M_step(Phi, gamma, alpha_old, documents, vocabulary, k)\n",
    "                \n",
    "        iteration += 1\n",
    "        \n",
    "        if iteration > 10:\n",
    "            break\n",
    "        \n",
    "    return Phi, gamma, alpha, Beta, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_init, beta_init, gamma_init, phi_init = initialize_parameters(corpus, vocabulary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document needed 43 iterations to converge.\n",
      "Document needed 48 iterations to converge.\n",
      "Document needed 45 iterations to converge.\n",
      "Document needed 38 iterations to converge.\n"
     ]
    }
   ],
   "source": [
    "phi, gamma, likelihood = E_step(phi_init, gamma_init, alpha_init, beta_init, corpus, vocabulary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M-step\n"
     ]
    }
   ],
   "source": [
    "alpha, beta = M_step(phi, gamma, alpha_init, corpus, vocabulary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational EM\n",
      "Likelihood==0\n",
      "Document needed 33 iterations to converge.\n",
      "Document needed 36 iterations to converge.\n",
      "M-step\n",
      "Document needed 40 iterations to converge.\n",
      "Document needed 39 iterations to converge.\n",
      "M-step\n",
      "Document needed 36 iterations to converge.\n",
      "Document needed 29 iterations to converge.\n",
      "M-step\n",
      "Document needed 12 iterations to converge.\n",
      "Document needed 9 iterations to converge.\n",
      "M-step\n",
      "Document needed 6 iterations to converge.\n",
      "Document needed 6 iterations to converge.\n",
      "M-step\n",
      "Document needed 5 iterations to converge.\n",
      "Document needed 6 iterations to converge.\n",
      "M-step\n",
      "Document needed 5 iterations to converge.\n",
      "Document needed 5 iterations to converge.\n",
      "M-step\n",
      "Document needed 5 iterations to converge.\n",
      "Document needed 5 iterations to converge.\n",
      "M-step\n",
      "Document needed 4 iterations to converge.\n",
      "Document needed 4 iterations to converge.\n",
      "M-step\n",
      "Document needed 4 iterations to converge.\n",
      "Document needed 4 iterations to converge.\n",
      "M-step\n"
     ]
    }
   ],
   "source": [
    "Phi, gamma, alpha, Beta, likelihood = \\\n",
    "        variational_EM(phi_init, gamma_init, alpha_init, beta_init, corpus, vocabulary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17781634, 0.17781634])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x7f411e49d438>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rappaport', 'bechtel', 'official', 'offer', 'peres']\n",
      "['shot', 'students', 'teacher', 'police', 'school']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXgc1Znv8e8rCduYxQbHZIiNkVmSicnchyEOS27u3DwhJCaTG5M7ZGKYO0BChizDczNZZkY8DFzCZDKBLDAJJIGELWbHEHBigyE2hN1YxoBtjLFsjC1sY3nfJMuS3vtHlaTuVrW6WqpWt6p/n+exVXXqVNU5XdVvnz5VfcrcHRERSa+achdARERKS4FeRCTlFOhFRFJOgV5EJOUU6EVEUk6BXkQk5WIFejObZmYrzazJzBoilo80s/vC5QvNrD5MrzezVjN7Jfz3q2SLLyIihdQVymBmtcCNwFlAM7DIzGa7++sZ2S4Gtrv7CWY2A7gG+GK4bLW7n5xwuUVEJKY4LfpTgSZ3X+Pu7cC9wPScPNOBO8LpWcCZZmbJFVNERAaqYIsemACsz5hvBk7Ll8fdO8xsJzAuXDbZzJYAu4B/c/dn+tvZe97zHq+vr49RLBER6bZ48eIt7j4+almcQB/VMs8dNyFfno3AJHffamYfBh42s5PcfVfWymaXAJcATJo0icbGxhjFEhGRbmb2dr5lcbpumoFjMuYnAhvy5TGzOmAMsM3d97v7VgB3XwysBt6fuwN3v9ndp7r71PHjIz+QRERkgOIE+kXAiWY22cxGADOA2Tl5ZgMXhtPnAgvc3c1sfHgxFzM7DjgRWJNM0UVEJI6CXTdhn/ulwDygFrjV3Zeb2dVAo7vPBm4BZppZE7CN4MMA4K+Aq82sA+gEvubu20pRERERiWaVNkzx1KlTXX30IiLFMbPF7j41apl+GSsiknIK9CIiKadALyKScgr0Vap5+z6eWrm53MUQkSGgQF+lPnXd01x026JyF0NEhoACfZXa195Z7iKIyBBRoBcRSTkFehGRlFOgFxFJOQV6EZGUU6AXEUk5BXoRkZRToBcRSTkFehGRlFOgFxFJOQV6EZGUU6AXEUk5BXoRkZRToBcRSTkFehGRlFOgFxFJOQV6EZGUU6AXEUk5BXoRkZRToBcRSTkFehGRlFOgFxFJOQV6EZGUU6AXEUk5BXoRkZRToBcRSTkFehGRlFOgFxFJuViB3symmdlKM2sys4aI5SPN7L5w+UIzq89ZPsnM9pjZd5MptoiIxFUw0JtZLXAjcDYwBTjPzKbkZLsY2O7uJwDXAdfkLL8OeHTwxRURkWLFadGfCjS5+xp3bwfuBabn5JkO3BFOzwLONDMDMLNzgDXA8mSKLCIixYgT6CcA6zPmm8O0yDzu3gHsBMaZ2SHAvwLf628HZnaJmTWaWWNLS0vcsouISAxxAr1FpHnMPN8DrnP3Pf3twN1vdvep7j51/PjxMYokIiJx1cXI0wwckzE/EdiQJ0+zmdUBY4BtwGnAuWZ2LTAW6DKzNne/YdAlFxGRWOIE+kXAiWY2GXgHmAGcn5NnNnAh8AJwLrDA3R34H90ZzOwqYI+CvIjI0CoY6N29w8wuBeYBtcCt7r7czK4GGt19NnALMNPMmgha8jNKWWgREYkvTosed58LzM1JuzJjug34QoFtXDWA8omIyCDpl7EiIimnQC8iknIK9CIiKadALyKScgr0IiIpp0AvIpJyCvQiIimnQC8iknIK9CIiKadALyKScgr0IiIpp0AvIpJyCvQiIimnQC8iknIK9CIiKadALyKScgr0IiIpp0AvIpJyCvQiIimnQC8iknIK9CIiKadALyKSclUV6B9btpHHlm0qdzFERIZUXbkLMJS+dufLAKz94V+XuSQiIkOnqlr0IiLVSIFeRCTlFOhFRFJOgV5EJOUU6EVEUk6BXkQk5RToRURSToFeRCTlYgV6M5tmZivNrMnMGiKWjzSz+8LlC82sPkw/1cxeCf+9amafT7b4IiJSSMFAb2a1wI3A2cAU4Dwzm5KT7WJgu7ufAFwHXBOmLwOmuvvJwDTgJjOrql/jioiUW5wW/alAk7uvcfd24F5gek6e6cAd4fQs4EwzM3ff5+4dYfoowJMotIiIxBcn0E8A1mfMN4dpkXnCwL4TGAdgZqeZ2XJgKfC1jMAvIiJDIE6gt4i03JZ53jzuvtDdTwI+AlxmZqP67MDsEjNrNLPGlpaWGEUSEZG44gT6ZuCYjPmJwIZ8ecI++DHAtswM7r4C2At8KHcH7n6zu09196njx4+PX/o8nlq5mZbd+we9HRGRNIgT6BcBJ5rZZDMbAcwAZufkmQ1cGE6fCyxwdw/XqQMws2OBDwBrEyl5Py66bRFfvOmFUu9GRGRYKHgHjLt3mNmlwDygFrjV3Zeb2dVAo7vPBm4BZppZE0FLfka4+seABjM7AHQB33D3LaWoSK41W/YOxW5ERCperFsd3X0uMDcn7cqM6TbgCxHrzQRmDrKMIiIyCKn7Zay77uAUEcmUukAvIiLZFOhFRFIudYFePTciItlSF+hFRCSbAr2ISMqlK9AvfxhvbuydX/YQvLO4fOVJyvKHYf2icpdCRIapdA0Z/MCF1AJwdzA/60vB36t2lqlACXkg/NHxcK+HiJRFulr0IiLShwK9iEjKKdCLiKScAr1ICu3Y187WPZU9VHdreycbdrSWuxhVQYFeJIVOvvoJPvz9P5a7GP268LaX+OgPF5S7GFVBgV5EyuKlt7YVziSJUKAXEUk5BXoRkZRToBcRSTkFehGRlFOgFxFJOQV6EZGUU6AXEUk5BXoRkZRToBcRSTkFehGRlFOgFxFJOQV6EZGUU6AXEUm5qgz09Q1zWPXu7nIXQ0RkSFRloAd4aa2GSBWR6lC1gV5EpFoo0IuIpJwCvYhIysUK9GY2zcxWmlmTmTVELB9pZveFyxeaWX2YfpaZLTazpeHfTyRbfBERKaRgoDezWuBG4GxgCnCemU3JyXYxsN3dTwCuA64J07cA/8vd/wK4EJiZVMEHy73cJRARGRpxWvSnAk3uvsbd24F7gek5eaYDd4TTs4AzzczcfYm7bwjTlwOjzGxkEgUXEZF44gT6CcD6jPnmMC0yj7t3ADuBcTl5/gZY4u77B1ZUEREZiLoYeSwiLbfjo988ZnYSQXfOpyJ3YHYJcAnApEmTYhRJRETiitOibwaOyZifCGzIl8fM6oAxwLZwfiLwO+ACd18dtQN3v9ndp7r71PHjxxdXAxER6VecQL8IONHMJpvZCGAGMDsnz2yCi60A5wIL3N3NbCwwB7jM3Z9LqtBFe/wK2Lu1Z/artb9nzO6mshVHRGQoFQz0YZ/7pcA8YAVwv7svN7OrzexzYbZbgHFm1gR8G+i+BfNS4ATgCjN7Jfx3VOK1KOT5n8Gj/wxADV1cdtA9THvx74a8GCIi5RCnjx53nwvMzUm7MmO6DfhCxHrfB74/yDImo7M9a7a2U9eERaQ66JexIiIpp0AvJbd2y14+8ZOn2LIn+Bb15dsX8dDLzWUulUj1UKCXkrvl2bdY07KXuUs3ArDgjc18+/5Xy1wqkepRtYHe+vwUQEQknaoo0Ae/6VJ4F5FqU0WBXkSkOinQy5DRiKEi5aFALyVnUSMhiciQqbpAr5gjItWm6gK9iEi1UaAXEUk5BXoRkZRToBcRSTkF+krx1A9h07LC+Z6/Ad5+ofTlKYcDbTDnO9C6vdwlEUmVqg30FTUEQucBeOo/4TefLJz38cvhtmmlL1M5vHo3LPoNLKiMka1F0qJ6Av1wuJm760C5S1Be3pX9V0QSUT2BXkSkSinQy5BxjYEgUhZVF+grqm++W8oDYNGdZil/PUSGWtUF+so2DK4jlFS111+kNBToRURSToFehowVvPNJXTYipaBAX1HSHeh0MVakPBTopYKoj16kFBToK4oCnYgkr4oCvYKoiFSnKgr0MnyoL18kSQr0FaE6AlvBWg6H8YhEhqGqC/QV+cvYbikNdIVvqxSRUkptoP94zSvlLkI8nQfgD9/Kv/y5/xq6spRbsbdfvv4ILL69JEWRKvHSr+GNueUuRcnVlbsApXL7iGvLXYR41jwFr9wVTEcFuieuHNLiDCv3XxD8/fBFZS2GDGNzvxv8vWpnectRYqlt0fcxgO6DA51dPLZs44B+6LN3fwcL3ni36PXSTB04IuWRmkC/d39H/xkGEKx/vqCJr935MvNXbC563X998DW+fHsjq1v2FL1uWulirEh5xAr0ZjbNzFaaWZOZNUQsH2lm94XLF5pZfZg+zsyeNLM9ZnZDskXP1tGZ/EXWDTtaAdi2t73odddu3QvAvv2d8VdSoBOREigY6M2sFrgROBuYApxnZlNysl0MbHf3E4DrgGvC9DbgCuC7iZU4b0FLvgcRkWEpTov+VKDJ3de4eztwLzA9J8904I5wehZwppmZu+9192cJAn5V8kq+nbNSafAzkUTFCfQTgPUZ881hWmQed+8AdgLjkihgXAV7PQbQLTKYLwmmrxgDoNdMpBTiBPqod19ukytOnvw7MLvEzBrNrLGlpSXuagULMJh8krzCDXW15EVKIU6gbwaOyZifCGzIl8fM6oAxwLa4hXD3m919qrtPHT9+fNzVJK10UVokUXEC/SLgRDObbGYjgBnA7Jw8s4ELw+lzgQWup0zIQOnUEUlUwV/GunuHmV0KzANqgVvdfbmZXQ00uvts4BZgppk1EbTkZ3Svb2ZrgcOBEWZ2DvApd3896YpoPJXKV/gQ6RiKlEKsIRDcfS4wNyftyozpNuALedatH0T5YiscIgYeRAZz54wapyL9c3c11EosNb+MLcV5MphtDmzddJ/s+tATKY/UBPrChjbKKKj1Kv5DTy+eSJKqKNBLucT+0NPXd5GSSGWgf33klyLTR9LOzw/6eUbGR+BPGcMZvzEHrhoDm5bC64/w8ZY7+VbdLD47/6wgfdbF+aNWx/4gz8KbgCBmfbJmMe9dUsR48p37448//8il0NUVf9vl8MeroGl+z+ypK38Ebz/fu3z57+DZ63rnh8PXoMcuy64DwL5twZDJrdvjbaN9LzxwEezelHjxpIRad8D9FwbHO9fSWXDX38Lj/1Z4O4tvh8ZbEy9ef1IZ6Efb/j5p7Z3OWTWLmVa7qDfx/gvgyf8AoHn7Prj3fAC67p4B91/AZ969mW/WPcQhbeEbctmsnjdza3snW/Zk7Gf1guDvo//Sk/SbET/hz17+aVY5duxrZ+Wm3fkL/8SVtOzuW/4+lsyEXc2Ri/bs72DHvuiB2NoOdObd/ru72mjvSPDD49nr4M7/3TP7ofV3wW1n9y5/4KLgw6CP3pb95l1t7O8oYmC4UnvxF9l16E57/ZHgIRZxLHsw+JBb8O/Jl09KZ9Gv4fWH4YWI8RkfvBhWzYPnexuSzdv3RW/n99/s/2FDJZDKQB9l3vL8rac5r23kY9c82TO/cWdrwe19/hfPMfX7fyy6HCdf/QSfvv5pNu/KP/zPR/6j+O1mOv0H8zn56icil/39LQsjt9/e0cVpP5jPP896dVD7TtqpP5jPP961pNzFECnKI6+8w8eueZLnm7aUuyhAFQX6/rzavCNrvt/nyobdC2/ktspzuh0K9TZvy9PiLkqero49/YzNv2htdPdCR9gN9Pjy5B+WMtiLsX9cMUwe4FJs19Mw6KkaCsOhxy5LjAIvWRfElD5xokxSE+jjnCwe8/bFJC4JFipOpZ7cZR1tc9hejC223MO1nlKsSnmbpybQD4bedhptUyRJldZmUaCP0G/XTextiEi6DN93ddUE+rjdNlAo0OdbVo4vaZXyxTBhldqvVVCx5R6u9ax2w++4VU2gh34Oj/U7W1VKEWNT3y1U7Pf0SvteLyVTKYP4VlWgjyvOXTcF0wu8mSvk+PeoqNhTUYUpQtF33VTYSSCplZpAX+gtk0S/e1GG6Zu4FKXWc3NFyis1gT5Z/QSmSmptDtMPk4LSWi+JNGyOdiW994tUNYHesbwXZHP7kPs9nHGD0DA+KcpnuL5muo9eslXadamqCfTFGFg3z7Bplwy5gZ70lXIhKz7ddSOVSYFesin2iPRv2DVAUhTok2z9DcWF22QuUA6/Ey6etNZLhrfK6o4pRmoCPZ0H+l38udoX+OWI6LHeGxaextpR5/fMH2l78m/oxyfAVWM4mDauqbs5GIP++ZxhS68awyFdGdtY92KQ76oxrB11Pk+O+BbWuR9+8kG469w+u1g76vwgf3Njv3WiZWXwd9dG+O058MrdcNUYfn3Qj1k+8ktw+2eDsfXvOQ86gkHUbjzoetaOOp+1o87nNFsBb84Lhk3NtKcFfjsd9m7NTt/5Dsz8PDTeBvMu779sGa5cfAa1ZA81fHXdbflXWHInvPUM3Pk3TLQWLq+7Mxjvu9vTPwpenyidB3qfHdDtscvgtQeC12HT0uz8bz0DD/5Dditt4U3wzE+D9LeegTVPwe++lr3e3HA46oe+Cm89nb8u7jDry3Dt8b3Hs/v6zav3RB/jV+8L6rD49qAea5/Lv/2BerQhGCoZguP623OgbWcw37IyGFv9QP4RVmNbchfML2I45lfuhvlXF8732gPw+BW985tXwN1fDJ4L0bE/KP/mN/rfRtQ5PO/y3nPtnZfhvv8DnfkHCcwn8hJd267g/VMGqQn0+za8PqT7+2TNy3yx7qlg5vHLeWpl9giLx2/KeJb63X+btWxyzbv8v1/MhN0b+t3Hxl9HPm+9x64HLqW+YQ48dz2seRIe/joAZ9W+zCG2H9Y+Q/Mvz4GVc/nqj2/nvJtf5K9rX+pZ/44RPwzKtvh2/vyKxwBo7+zihh9+B9Y8xY9+0EB9wxzef/mjwQpPXxuMu/+Hf4IXbmDd1n3s3HeA+oY5PNC4nrN++ifqG+Zw45NNPLsqe3jWYy379bmgrncY5cseWkp9wxz+9aGMIHzHZ7HV8/l23QP8Q91cePBi6hvmBPVd8P38L8qWN4O/yzI+GF78BTz0FVg5l/YHv059wxz+9GZLsGzmObD0fugK3sz3vLQueKbA/O8F6TPPCT70Xr0nez8vBQ+Y4bV7YV34IBJ3Xl2/g/qGOSx7JwyaHW3B+PP7tgQPrcj1wEV90353SfD3998M6hE2Brq6nPqGOfx8/qo+q1zz2BvBaxPXwl/27vvpa4PzZ9mDwfyc7wRjqze/lHf1/3vPEj4YnjPd7l64jvqGOdmjpz7yDXjmx9Q3zKHhwdcKl+vhr8MzPymc76GvwPM/653//TfhzceC4Lx+YVD+ud/tfxtR48q/cEMwtjwEf1f8Hna8Xbg8cSx/qPe5FaH12/ZR3zCHJ9/YnMw+8khNoN8cYwz5UrrnpfVZ88mMl9P/NloPxG9pbNrZygtrthbOGLH/9s7oh5EsXreNdduChyvc/vxaVm0OvsX81/xVPLQk+qEoUe55aV1R5RqMfe3Ba/a7l3PKF7bof/PMmsj0uB5/PXjuQfQbN2JbsYZdDfJ0hn+vjwj0v3xqdewyFtpPHLNf3UDrgexvad2v3aad0d8E7l20PjI92YvuJej261O+5PbxyvpgOONZi+O/XwYiNYG+snt1+36PS6K8FjE1sPWz5R8XqG96vrtIk7q9rPgPzMHd6mjF3BYbEaD61ju5ft3S9RDn2XKxAbisXdjWd3rQHyCDr1B2EeK/f5KWnkBfYVfCK+2yzUDKUyjIZr7kWS9/xGpD/svkvDzj/77p+fIHk/HrkGzO7DylO9eT2m6lHOukFF+fYt9vpf71eGoC/VDf8pR7IGv6O7IRH9vFjKaZvwyed/vxRL9m+Qd/y7+f7DjvAypS1H6L/oCIueOe0yUnf1E/lovzlT6yPAP8XUH+vQxO3tesuD11b6Usba7MOvRMD7Igg2huR+45YntD9cOq1AT6riE/u/p/dOBQ9NEXcyJHbaumwPqFTsEK+xIVi8UdlC4yvYh2elTWqMQi+uhLzgfbcKgApXithuB1KfUhTk2gH+qBs4oJ5FGt9zgt+kI5Bt9Hn69FX3wffWZ3gntyXVfFbydmi75P/iCl3/dyjBZ8n/UTfAd3byr5oJBMH3339Y3yfP4PYR99oe26Z5xVmXnVRz94Q3x2FbrkNjQt+sFtq3CLvkAf/aBKVGHyvnkz++ij7z6KXiuySR8zbSB5kjSMW/QlGn+11NSij2moL8bmBsHcPvqs2QF+bBcKtIPto6+xfNu3cLOFX9OoPkYfeJEitp90H314MbbP1/GwRd9vkCvQR+/R39365M8sYxFdN3G+tQ7ofWD5WsBD00efyDs3sw5J99En9ZyByD76oZGaQD/0F2NLv78a+m9BVs6dLClQSX3lIglLTaAf+j76nPk+Lfr+++aS7aMvrLhrCnm2H9UiydPo6Tv088COT9L30ffdXhF99H3WjXGXTcGLucl23QzssyhPC7joPvqB7DspmXVIuo8+9x0R45tuZBHURz9oQ951k9OtUYrjVahFX27u+W+jzE0vdD2g7GK16Asfj1LeYhiriEnsaMBdjcO5bz9JxRyF7gvYpX1/xAr0ZjbNzFaaWZOZNUQsH2lm94XLF5pZfcayy8L0lWb26eSKnqPsXTd9brfoZ1lSLfr4ffRFteg9X8ulv/voM++6Kf7Cb7CNBPowC70WPf3dufkH0HUT65yL6KPPrFXCXUbl7KPvXasMH+qV1EdfaHtlUDDQm1ktcCNwNjAFOM/MpuRkuxjY7u4nANcB14TrTgFmACcB04BfhNtLXLm7T/Nf2By4JPrge4N28oIvydHb79uir5BvJwVejqKGQOij9xtOuU7HZPY7wBa9GvSBIoJR0p8l+cRp0Z8KNLn7GndvB+4FpufkmQ7cEU7PAs604B0zHbjX3fe7+1tAU7i9EqjcrgGP/GVsYbHvuum3pR1XRos83/b6+2VsZsM1atXY5RismHvqbdKH8/29Ugm1EAf9S+YYexhUH305y5DY3inZrziKOG7RDYbyfRJaoa96ZnYuMM3dvxLO/z1wmrtfmpFnWZinOZxfDZwGXAW86O53hum3AI+6+yzymDp1qjc2FhiHPcKf7v8Z//P1KwpnLJEut6Ja9Vv9MMbZ7qL2sbbmGOq7skcAfLNrAu+veafgurt8NJv8iLx5V3VN6AljmXne7JoAwIi6msh919UaHZ3R9c7czrqu8UyqaYksc/c+CtUjX763a47pKftob+UoD4ZIXltzDEBkuXPrtK5mAl3U0N7Rlbcc62omMKkro041E5nUlT3qYPe2u7dfg2fliTqG3eXslrs8M097R1fPtjPlpufLl7uP3PJkzrcxgk01741cP2r73Wm1NUZteL9x97YyX/P+tpFZrv7k5uueb2UUB+wgDvfdBbeTW+/c7XZPb7Uj2G2HRuaP2tbbNcewv6P3G2x3/aKO+4HOrp4PxhF1NWwa/zFO//qv+q17Pma22N2nRi2ri7N+RFqcztt8H619ooKZXQJcAjBp0qQYRerr+FM+AQkG+s0+lqNsR97lj3V+hGm1iwDY6yNZPurDnLr/+Z7lT3Sewkk1a3mfbWPdyBOZtD97aNmFXR/kM7X5x/sGWN51LCfVvA1Aa81oto2eTP2e3pNlac0HWNc5hgOjJ3DSvpfYXjeeIzpasraxqOv9fKTmTZ7t+hB1NTW8n95AtbJrInV0cnzNRt703gC1vvMozqxdwoLOk2llBABHjx7Fvq73MmVfIztqjmBs13Z2H3Y8AJt2tTH24BHsajtAlzuHjaxj1EG10Nq7r6U+mUm0sLrraFb5hKxyvFM3iX0HOnmr82g+XRt8yK+pO47jOtbwfOcUPlr7Ojtrj2RVZ1DGY20LI30/O2rHMbZzK1tHT+7Z1jbgqD3Bg0C2hen1e9b35H1r5J+zat/hjDtkJCPqjLr2g5jYvoYto4O6tB3o4n1tWznUgqF2N4w4FjDe176WLaOPZ9Ke3nJvGX0ck/Y0s6XuaN7TsZFlo0+llVFs3t3GUYeN7Al2k/YEgX7dyBPYdtD72OGTOHnvc1lpmca27mNs51b2MJpD2cc7IyazbUQQWDbubOOwkXUcOir7rbu7rYM9+zs4evQoAFp276ejy3vmc3W/JtsOntxzXF895KN0Wh0HOsZyYttSVhxyWuS3UYDtew/Q1tGZtf3W9i52tLbzZ4eM6mn8Htm6h8M7t7PKJ3DwQbWMHX1Q3jIDHNG6hzGd23uOXT5j2lo5omNLT76OjrGc0LaUFYcGHQan7Hma1aNOYmfduLzb6H4v7a05LOtc2Vl7JNsOnox3HMrkthW8fchfALDLJ/Lf9r7A8tGnsr8m+3Ud17qLwzqDZxBsHT2Zjk6nZc9+3nvYKGrCz7HM495FDdtGT8Y9eP+MO2RE8IFwePa5kJQ4LfozgKvc/dPh/GUA7v6fGXnmhXleMLM6YBMwHmjIzJuZL9/+BtqiFxGpZv216OP00S8CTjSzyWY2guDi6uycPLOBC8Ppc4EFHnyCzAZmhHflTAZOBPpvxoqISKIKdt24e4eZXQrMA2qBW919uZldDTS6+2zgFmCmmTURfHueEa673MzuB14HOoB/dPfOyB2JiEhJFOy6GWrquhERKd5gu25ERGQYU6AXEUk5BXoRkZRToBcRSTkFehGRlKu4u27MrAV4exCbeA+wJaHiVDLVM11Uz3QpRz2PdffxUQsqLtAPlpk15rvFKE1Uz3RRPdOl0uqprhsRkZRToBcRSbk0Bvqby12AIaJ6povqmS4VVc/U9dGLiEi2NLboRUQkQ2oCfaEHmFc6MzvGzJ40sxVmttzMvhmmH2lmT5jZqvDvEWG6mdnPwvq+ZmanZGzrwjD/KjO7MN8+y8nMas1siZn9IZyfHD5YflX4oPkRYXr5Hzw/QGY21sxmmdkb4XE9I43H08y+FZ6zy8zsHjMblZbjaWa3mtnm8Cl63WmJHUMz+7CZLQ3X+ZlZiZ4z6e7D/h/B8MmrgeOAEcCrwJRyl6vIOhwNnBJOHwa8SfAw9muBhjC9AbgmnP4M8CjBU7xOBxaG6UcCa8K/R4TTR5S7fhH1/TZwN/CHcP5+YEY4/Svg6+H0N4BfhdMzgPvC6SnhcR4JTA6Pf2256zzY0I0AAAM1SURBVJVTxzuAr4TTI4CxaTuewATgLeDgjON4UVqOJ/BXwCnAsoy0xI4hwfM5zgjXeRQ4uyT1KPcLmdDBOAOYlzF/GXBZucs1yDo9ApwFrASODtOOBlaG0zcB52XkXxkuPw+4KSM9K18l/AMmAvOBTwB/CE/yLUBd7vEkeA7CGeF0XZjPco9xZr5K+AccHgZAy0lP1fEMA/36MIjVhcfz02k6nkB9TqBP5BiGy97ISM/Kl+S/tHTddJ9s3ZrDtGEp/Dr7l8BC4L3uvhEg/HtUmC1fnYfDa3E98C9A9xOUxwE73L0jnM8sc099wuU7w/yVXs/jgBbgtrCL6jdmdggpO57u/g7wY2AdsJHg+CwmfcczU1LHcEI4nZueuLQE+lgPIR8OzOxQ4EHgn9x9V39ZI9JiP5C9XMzss8Bmd1+cmRyR1Qssq+h6ErRWTwF+6e5/CewlfIZyHsOynmH/9HSC7pb3AYcAZ0dkHe7HM45i6zZkdU5LoG8GjsmYnwhsKFNZBszMDiII8ne5+0Nh8rtmdnS4/Ghgc5ier86V/lr8d+BzZrYWuJeg++Z6YKwFD5aH7DL31CdcPobgcZWVXs9moNndF4bzswgCf9qO5yeBt9y9xd0PAA8BHyV9xzNTUsewOZzOTU9cWgJ9nAeYV7TwavstwAp3/2nGoswHr19I0HffnX5BeKX/dGBn+DVyHvApMzsibG19KkyrCO5+mbtPdPd6guO0wN3/DniS4MHy0Leew+7B8+6+CVhvZh8Ik84keHZyqo4nQZfN6WY2OjyHu+uZquOZI5FjGC7bbWanh6/dBRnbSla5L3QkeMHkMwR3qqwGLi93eQZQ/o8RfG17DXgl/PcZgv7L+cCq8O+RYX4DbgzruxSYmrGtLwNN4b8vlbtu/dT54/TedXMcwRu7CXgAGBmmjwrnm8Llx2Wsf3lY/5WU6G6FQdbvZKAxPKYPE9xxkbrjCXwPeANYBswkuHMmFccTuIfg2sMBghb4xUkeQ2Bq+LqtBm4g5+J9Uv/0y1gRkZRLS9eNiIjkoUAvIpJyCvQiIimnQC8iknIK9CIiKadALyKScgr0IiIpp0AvIpJy/x+QjRKuwvbwUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for l in range(k):\n",
    "    beta_topic = beta[l,:]\n",
    "    beta_topic_top4 = np.argsort(beta_topic)[-5:]\n",
    "    plt.plot(beta_topic)\n",
    "    print([w for w in np.array(vocabulary)[beta_topic_top4][:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITE TEXT WITH COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A gunman took a yearold woman hostage after he was foiled in an attempt to steal  million in jewelry belonging to the late Liberace but police shot and killed the man outside the entertainers museum I just tried to stay cool said hostage Margaret Bloomberg who sat down to give police a clear shot at the man and escaped unharmed in Sunday evenings incident at the Liberace Museum The man had a bag of tools including a crowbar and was going to smash into the jewelry case said Dora Liberace administrator of the museum and sisterinlaw of the late'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_full_text(corpus_file, number_of_text):\n",
    "    fulltext_words=[]\n",
    "    fulltext_allwords=[]\n",
    "    #number_of_text=2\n",
    "    text_counter=0\n",
    "    special_chars = '1234567890~!@#£$%^&*()_+,./<>?\\|\"]}\\'[{`-'\n",
    "\n",
    "    with open(corpus_file, 'r') as text:\n",
    "        new=False\n",
    "        for line in text:\n",
    "            if new:\n",
    "                #print(line.strip()[0], line.strip()[:10])\n",
    "                if line.strip()[0]==\"<\":\n",
    "                    pass\n",
    "                else:\n",
    "                    #print(\"FOUND\", text)\n",
    "                    text_counter+=1\n",
    "                    if text_counter==number_of_text:\n",
    "                        #print(\" CORRECT\")\n",
    "                        new_text=line\n",
    "                        fulltext=new_text\n",
    "                        words = np.array(new_text.split())\n",
    "                        for word in words:\n",
    "                            fulltext_allwords.append(word)\n",
    "                            for char in special_chars: # remove punctuation etc,\n",
    "                                word = word.replace(char, '') \n",
    "                            fulltext_words.append(word)\n",
    "\n",
    "\n",
    "            else:\n",
    "                if line.strip() == \"<TEXT>\":\n",
    "                    new=True\n",
    "    return fulltext_words, fulltext_allwords\n",
    "\n",
    "fulltext_words, fulltext_allwords = get_full_text('ap/ap.txt', 3)\n",
    "\" \".join(fulltext_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yearold= 1, million= 0, police= 1, shot= 1, killed= 1, man= 0, outside= 1, police= 1, shot= 1, man= 0, man= 0, came= 0, take= 0, man= 0, saying= 0, man= 0, gun= 1, offered= 0, offered= 0, dont= 1, know= 1, door= 1, police= 1, outside= 1, went= 1, police= 1, told= 0, door= 1, police= 1, door= 1, gun= 1, told= 0, police= 1, didnt= 1, police= 1, shot= 1, police= 1, work= 0, thought= 0, identified= 1, died= 1, ago= 1, students= 1, "
     ]
    }
   ],
   "source": [
    "colors=['blue','green', 'red']\n",
    "fulltext_colors=[]\n",
    "\n",
    "how_significant=1.9\n",
    "significance=[]\n",
    "for word in fulltext_words:\n",
    "    if word in vocabulary:\n",
    "        v = np.where(vocabulary==word)[0][0]\n",
    "        #print(v,word_beta )\n",
    "        word_beta = beta[:,v]\n",
    "        #significance.append(np.max(word_beta)/np.mean(word_beta))\n",
    "        if np.max(word_beta)>np.mean(beta):\n",
    "            if np.max(word_beta)> how_significant*np.mean(word_beta):\n",
    "                #significance =  (np.max(word_beta) / np.mean(word_beta) > 10)\n",
    "                topic = np.where(np.max(word_beta)==word_beta)[0][0]\n",
    "                color = colors[topic]\n",
    "                print(word+\"=\",str( topic)+\", \", end=\"\")\n",
    "\n",
    "            else:\n",
    "                color='k'\n",
    "        else:\n",
    "            color='k'\n",
    "    else: \n",
    "        color='k'\n",
    "    fulltext_colors.append(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mA \u001b[0mgunman \u001b[0mtook \u001b[0ma \u001b[0;32;48m74-year-old \u001b[0mwoman \u001b[0mhostage \u001b[0mafter \u001b[0mhe \u001b[0mwas \u001b[0mfoiled \u001b[0min \u001b[0man \u001b[0mattempt \u001b[0mto \u001b[0msteal \u001b[0m$1 \u001b[0;34;48mmillion \u001b[0min \u001b[0mjewelry \u001b[0mbelonging \u001b[0mto \u001b[0mthe \u001b[0mlate \u001b[0mLiberace, \u001b[0mbut \u001b[0;32;48mpolice \u001b[0;32;48mshot \u001b[0mand \u001b[0;32;48mkilled \u001b[0mthe \u001b[0;34;48mman \u001b[0;32;48moutside \u001b[0mthe \u001b[0mentertainer's \u001b[0mmuseum. \u001b[0m``I \u001b[0mjust \u001b[0mtried \u001b[0mto \u001b[0mstay \u001b[0mcool,'' \u001b[0msaid \u001b[0mhostage \u001b[0mMargaret \u001b[0mBloomberg, \u001b[0mwho \u001b[0msat \u001b[0mdown \u001b[0mto \u001b[0mgive \u001b[0;32;48mpolice \u001b[0ma \u001b[0mclear \u001b[0;32;48mshot \u001b[0mat \u001b[0mthe \u001b[0;34;48mman \u001b[0mand \u001b[0mescaped \u001b[0munharmed \u001b[0min \u001b[0mSunday \u001b[0mevening's \u001b[0mincident \u001b[0mat \u001b[0mthe \u001b[0mLiberace \u001b[0mMuseum. \u001b[0m``The \u001b[0;34;48mman \u001b[0mhad \u001b[0ma \u001b[0mbag \u001b[0mof \u001b[0mtools, \u001b[0mincluding \u001b[0ma \u001b[0mcrowbar, \u001b[0mand \u001b[0mwas \u001b[0mgoing \u001b[0mto \u001b[0msmash \u001b[0minto \u001b[0mthe \u001b[0mjewelry \u001b[0mcase,'' \u001b[0msaid \u001b[0mDora \u001b[0mLiberace, \u001b[0madministrator \u001b[0mof \u001b[0mthe \u001b[0mmuseum \u001b[0mand \u001b[0msister-in-law \u001b[0mof \u001b[0mthe \u001b[0mlate \u001b[0mentertainer. \u001b[0m``He \u001b[0mwanted \u001b[0mthe \u001b[0mjewelry \u001b[0mand \u001b[0mhe \u001b[0;34;48mcame \u001b[0mprepared \u001b[0mto \u001b[0;34;48mtake \u001b[0mit.'' \u001b[0mMrs. \u001b[0mBloomberg, \u001b[0mwho \u001b[0mhas \u001b[0mworked \u001b[0mat \u001b[0mthe \u001b[0mmuseum \u001b[0m10 \u001b[0myears, \u001b[0mwas \u001b[0mclosing \u001b[0mthe \u001b[0moffice \u001b[0mwhen \u001b[0mthe \u001b[0;34;48mman \u001b[0mappeared, \u001b[0;34;48msaying \u001b[0mhe \u001b[0mwanted \u001b[0mto \u001b[0mdeliver \u001b[0ma \u001b[0mplant, \u001b[0mMrs. \u001b[0mLiberace \u001b[0msaid. \u001b[0mThe \u001b[0;34;48mman \u001b[0mproduced \u001b[0ma \u001b[0;32;48mgun, \u001b[0mforced \u001b[0mhis \u001b[0mway \u001b[0minside \u001b[0mand \u001b[0mrefused \u001b[0moffers \u001b[0mof \u001b[0mmoney, \u001b[0mMrs. \u001b[0mBloomberg \u001b[0msaid. \u001b[0m``Margaret \u001b[0;34;48moffered \u001b[0mhim \u001b[0mthe \u001b[0mday's \u001b[0mreceipts, \u001b[0meven \u001b[0;34;48moffered \u001b[0mhim \u001b[0mthe \u001b[0mmoney \u001b[0min \u001b[0mher \u001b[0mpurse, \u001b[0mbut \u001b[0mhe \u001b[0mwasn't \u001b[0minterested,'' \u001b[0mMrs. \u001b[0mLiberace \u001b[0msaid. \u001b[0m``He \u001b[0msaid \u001b[0m`I \u001b[0;32;48mdon't \u001b[0mwant \u001b[0mthe \u001b[0mcash. \u001b[0mI \u001b[0mwant \u001b[0mthe \u001b[0mjewelry.' \u001b[0mHe \u001b[0mobviously \u001b[0mhad \u001b[0mbeen \u001b[0min \u001b[0mthere \u001b[0mbefore \u001b[0mand \u001b[0mchecked \u001b[0mout \u001b[0mthe \u001b[0mplace. \u001b[0mHe \u001b[0mseemed \u001b[0mto \u001b[0;32;48mknow \u001b[0mwhere \u001b[0meverything \u001b[0mwas.'' \u001b[0mMrs. \u001b[0mBloomberg \u001b[0mwas \u001b[0mable \u001b[0mto \u001b[0mwarn \u001b[0ma \u001b[0mcleaning \u001b[0mwoman, \u001b[0mwho \u001b[0mslipped \u001b[0mout \u001b[0ma \u001b[0mback \u001b[0;32;48mdoor \u001b[0mand \u001b[0mcalled \u001b[0;32;48mpolice. \u001b[0mThe \u001b[0mgunman \u001b[0mtied \u001b[0mMrs. \u001b[0mBloomberg's \u001b[0mhands \u001b[0mand \u001b[0mfeet, \u001b[0mtaped \u001b[0mher \u001b[0mmouth, \u001b[0mthen \u001b[0muntied \u001b[0mher, \u001b[0mmoved \u001b[0mher \u001b[0mto \u001b[0manother \u001b[0mpart \u001b[0mof \u001b[0mthe \u001b[0mmuseum \u001b[0mand \u001b[0mtied \u001b[0mher \u001b[0magain. \u001b[0m``He \u001b[0mwas \u001b[0mgetting \u001b[0mready \u001b[0mto \u001b[0mpop \u001b[0mthe \u001b[0mjewelry \u001b[0mcase \u001b[0mwhen \u001b[0mhe \u001b[0mheard \u001b[0ma \u001b[0mnoise \u001b[0;32;48moutside,'' \u001b[0mMrs. \u001b[0mBloomberg \u001b[0msaid. \u001b[0m``He \u001b[0;32;48mwent \u001b[0mand \u001b[0msaw \u001b[0mthe \u001b[0;32;48mpolice. \u001b[0mI \u001b[0;34;48mtold \u001b[0mhim \u001b[0mI'd \u001b[0mget \u001b[0mhim \u001b[0mout \u001b[0mthe \u001b[0mback \u001b[0;32;48mdoor. \u001b[0mWhen \u001b[0mwe \u001b[0mtried \u001b[0mthat, \u001b[0;32;48mpolice \u001b[0mwere \u001b[0mthere, \u001b[0mtoo.'' \u001b[0mThe \u001b[0mgunman \u001b[0mwalked \u001b[0mout \u001b[0mthe \u001b[0mfront \u001b[0;32;48mdoor \u001b[0mwith \u001b[0ma \u001b[0;32;48mgun \u001b[0mpointed \u001b[0mat \u001b[0mthe \u001b[0mbound \u001b[0mhostage. \u001b[0m``He \u001b[0;34;48mtold \u001b[0;32;48mpolice \u001b[0mhe \u001b[0mwould \u001b[0mshoot \u001b[0mme \u001b[0mif \u001b[0mthey \u001b[0;32;48mdidn't \u001b[0mlet \u001b[0mhim \u001b[0mget \u001b[0mto \u001b[0mhis \u001b[0mcar,'' \u001b[0mshe \u001b[0mrecalled. \u001b[0m``I \u001b[0mjust \u001b[0mtried \u001b[0mto \u001b[0mstay \u001b[0mcool.'' \u001b[0m``She \u001b[0mjust \u001b[0msat \u001b[0mdown \u001b[0mon \u001b[0mthe \u001b[0msidewalk, \u001b[0mpretending \u001b[0mher \u001b[0mlegs \u001b[0mhad \u001b[0mcollapsed \u001b[0munder \u001b[0mher,'' \u001b[0mMrs. \u001b[0mLiberace \u001b[0msaid. \u001b[0m``He \u001b[0mtried \u001b[0mto \u001b[0mlift \u001b[0mher \u001b[0min \u001b[0mthe \u001b[0mcar \u001b[0mbut \u001b[0mcouldn't. \u001b[0mHe \u001b[0mleaned \u001b[0mback \u001b[0mfor \u001b[0ma \u001b[0mminute \u001b[0mand \u001b[0mthe \u001b[0;32;48mpolice \u001b[0;32;48mshot \u001b[0mhim.'' \u001b[0m``When \u001b[0mI \u001b[0msat \u001b[0mdown \u001b[0mI \u001b[0mfigured \u001b[0mthe \u001b[0;32;48mpolice \u001b[0mwould \u001b[0mpick \u001b[0mhim \u001b[0moff, \u001b[0mmaybe,'' \u001b[0mMrs. \u001b[0mBloomberg \u001b[0msaid. \u001b[0mMrs. \u001b[0mBloomberg, \u001b[0msomewhat \u001b[0mshaken \u001b[0mand \u001b[0mher \u001b[0mhands \u001b[0mstill \u001b[0mbearing \u001b[0mmarks \u001b[0mfrom \u001b[0mthe \u001b[0mropes, \u001b[0mwas \u001b[0mback \u001b[0mat \u001b[0;34;48mwork \u001b[0mMonday. \u001b[0m``I \u001b[0;34;48mthought \u001b[0mI \u001b[0mwas \u001b[0mbetter \u001b[0moff \u001b[0mworking \u001b[0mthan \u001b[0mstaying \u001b[0mat \u001b[0mhome \u001b[0mand \u001b[0mdwelling \u001b[0mon \u001b[0mit,'' \u001b[0mshe \u001b[0msaid. \u001b[0mThe \u001b[0mgunman, \u001b[0;32;48midentified \u001b[0mas \u001b[0mHugh \u001b[0mPerry, \u001b[0m47, \u001b[0mof \u001b[0mLas \u001b[0mVegas, \u001b[0mhad \u001b[0ma \u001b[0mlengthy \u001b[0marrest \u001b[0mrecord \u001b[0mdating \u001b[0mback \u001b[0mto \u001b[0mthe \u001b[0m1960s, \u001b[0msaid \u001b[0mMetro \u001b[0mPolice \u001b[0mLt. \u001b[0mKyle \u001b[0mEdwards. \u001b[0mThe \u001b[0mmuseum \u001b[0mis \u001b[0mone \u001b[0mof \u001b[0mthe \u001b[0mcity's \u001b[0mtop \u001b[0mtourist \u001b[0mattractions, \u001b[0mfeaturing \u001b[0mmemorabilia \u001b[0mof \u001b[0mthe \u001b[0mentertainer \u001b[0mwho \u001b[0mgained \u001b[0mfame \u001b[0mhere \u001b[0mand \u001b[0mretained \u001b[0ma \u001b[0mhome \u001b[0mnot \u001b[0mfar \u001b[0maway. \u001b[0mMrs. \u001b[0mLiberace \u001b[0msaid \u001b[0mthe \u001b[0mmuseum \u001b[0mhas \u001b[0mone \u001b[0mof \u001b[0mthe \u001b[0mmost \u001b[0msophisticated \u001b[0msecurity \u001b[0msystems \u001b[0min \u001b[0mthe \u001b[0mcity \u001b[0mbut \u001b[0mwill \u001b[0mnow \u001b[0madd \u001b[0mguards. \u001b[0mLiberace, \u001b[0mwho \u001b[0;32;48mdied \u001b[0min \u001b[0mFebruary \u001b[0m1987 \u001b[0mof \u001b[0mcomplications \u001b[0mdue \u001b[0mto \u001b[0mAIDS, \u001b[0mfounded \u001b[0mthe \u001b[0mmuseum \u001b[0m10 \u001b[0myears \u001b[0;32;48mago \u001b[0mto \u001b[0mfund \u001b[0mthe \u001b[0mLiberace \u001b[0mFoundation \u001b[0mfor \u001b[0mthe \u001b[0mPerforming \u001b[0mArts, \u001b[0mwhich \u001b[0mprovides \u001b[0mscholarships \u001b[0mfor \u001b[0mmusic \u001b[0mand \u001b[0mart \u001b[0;32;48mstudents \u001b[0mat \u001b[0m27 \u001b[0mcolleges \u001b[0mand \u001b[0muniversities \u001b[0macross \u001b[0mthe \u001b[0mUnited \u001b[0mStates. "
     ]
    }
   ],
   "source": [
    "#http://ozzmaker.com/add-colour-to-text-in-python/\n",
    "#print(\"Examples of how to use ANSI COLOR: \\033[1;37;40m White          \\033[0m 1;37;40m            \\033[0;37;40m Light Grey \\033[0m 0;37;40m               \\033[0;37;48m Black      \\033[0m 0;37;48m\")\n",
    "colors_ansi=[34, 32,31] #blue, green, red\n",
    "             \n",
    "             \n",
    "for a in range(len(fulltext_allwords)):\n",
    "    if fulltext_colors[a]=='k':\n",
    "        #IF WE WANT TO FOCUS ON THE TOPIC WORDS:\n",
    "        #print(\"\\033[0;37;48m\"+fulltext_allwords[a], end=\" \")\n",
    "        print(\"\\033[0m\"+fulltext_allwords[a], end=\" \")\n",
    "        \n",
    "        #print(fulltext_allwords[a], end=\" \")\n",
    "    else:\n",
    "        for j in range(k):\n",
    "             if fulltext_colors[a]==colors[j]:\n",
    "                print(\"\\033[0;\"+str(colors_ansi[j])+\";48m\"+fulltext_allwords[a], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mTOPIC: 0: \u001b[0;34;48mmillion, man, man, man, came, take, man, saying, man, offered, offered, told, told, work, thought\n",
      "\u001b[0m\n",
      "\u001b[0mTOPIC: 1: \u001b[0;32;48myearold, police, shot, killed, outside, police, shot, gun, dont, know, door, police, outside, went, police, door, police, door, gun, police, didnt, police, shot, police, identified, died, ago, students\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "topic_words =[]\n",
    "for i in range(k):\n",
    "    topic_words.append(np.where(np.array(fulltext_colors)==colors[i]))\n",
    "    print(\"\\033[0mTOPIC: \"+str(i)+\": \"+\"\\033[0;\"+str(colors_ansi[i])+\";48m\"+ \", \".join(np.array(fulltext_words)[topic_words[i]]))\n",
    "    print(\"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
