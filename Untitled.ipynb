{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD2434_Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS:\n",
    "\n",
    "k - number of topics\n",
    "N - number of words in a document (different for each document)\n",
    "M - number of documents in a corpus\n",
    "\n",
    "Model parameters:\n",
    "z_n - [k] dimension vector; topic distribution for word n \n",
    "Theta - [k] dimension vector; mixture weights\n",
    "alpha - [k] dimension vector; prior probability for theta (mixture weights) (alpha > 0)\n",
    "beta - [k x V] dimension matrix; beta_ij = p(w^j = 1 | z^i = 1) \n",
    "                                 probability for a specific word j given a specific topic i\n",
    "D - list of [V x N] dimension matrices, that is M long = [\\mathbf{w}_1, ... \\mathbf{w}_M];\n",
    "                                 where \\mathbf{w} = [w_1,...,w_N] is [V x N] (one document consisting of N words) \n",
    "\n",
    "\n",
    "Variational parameters:\n",
    "Gamma - [k] dimension vector; determines Theta in the Variational Model\n",
    "Phi = phi_1 .. phi_N - [N x k] dimension matrix; determines the probability distribution \n",
    "                                 for topics z of words in the Variational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "from scipy import special\n",
    "from scipy.special import digamma, gammaln\n",
    "import pandas as pd\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Document Corpus\n",
    "Download the data from \n",
    "https://github.com/Blei-Lab/lda-c/blob/master/example/ap.tgz\n",
    "\n",
    "We can directly load the file \"ap.dat\" which contains:\n",
    "\n",
    "1 line = 1 document,\n",
    "\n",
    "[number of different words in doc] [word index (where the one is in w_n]:[how often it occurs in the doc] [word index 2]:[occurences 2] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = np.genfromtxt('ap/vocab.txt',  dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_data(corpus_file, vocabulary_file, stopwords_file):\n",
    "    \"\"\"\n",
    "    Reads the corpus from the .txt file into a list of lists;\n",
    "    a list for each document which contains a list of all the \n",
    "    words as strings.\n",
    "    Input parameters:\n",
    "    corpus_file - path to the corpus file\n",
    "    vocabulary_file - path to the vocabulary file\n",
    "    stopwords_file - path to the stopwords file\n",
    "    \"\"\"\n",
    "    vocabulary = np.genfromtxt(vocabulary_file,  dtype='str')\n",
    "    special_chars = '1234567890~!@#Â£$%^&*()_+,./<>?\\|\"]}\\'[{`-'\n",
    "    corpus = []\n",
    "    \n",
    "    # read in stopwords from file into a list\n",
    "    stopwords = [] \n",
    "    with open(stopwords_file, 'r') as file:\n",
    "        stop_words = file.read().replace(',', ' ')\n",
    "        for word in stop_words.split():\n",
    "            stopwords.append(word) \n",
    "    \n",
    "    with open(corpus_file, 'r') as text:\n",
    "        doc = ''\n",
    "        new = False\n",
    "        for line in text:\n",
    "            if new: # reached a new document\n",
    "                if line.strip() != '<TEXT>': # until we reach the new doc\n",
    "                    for char in special_chars: # remove punctuation etc,\n",
    "                        line = line.replace(char, '')\n",
    "                    doc += line\n",
    "                else: # we've reached a new doc again\n",
    "                    doc = doc.lower() # all words lowercase\n",
    "                    words = np.array(doc.split())\n",
    "                    doc = [word for word in words if word not in stopwords]\n",
    "                    corpus.append(doc)\n",
    "                    doc = ''\n",
    "            elif line.strip() == '<TEXT>': new = True\n",
    "\n",
    "    \n",
    "    return corpus, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocabulary = clean_up_data('ap/ap.txt', 'ap/vocab.txt', 'ap/stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(documents, vocabulary, k, M):\n",
    "    \n",
    "    # Initialize alpha\n",
    "    alpha = np.ones([M,k])*0.5 # for every document, for every topic\n",
    "    \n",
    "    # Initialize beta\n",
    "    beta = np.zeros([k,len(vocabulary)]) # for every topic, for every word in the vocabulary\n",
    "    for i in range(k):\n",
    "        beta[i,:] = np.random.uniform(0, 1, len(vocabulary)) / np.sum(np.random.uniform(0, 1, len(vocabulary)))\n",
    "    \n",
    "    # Initialize phi and gamma\n",
    "    phi = []\n",
    "    gamma = np.zeros([M,k]) # for every document, for every topic\n",
    "    for m in range(M):\n",
    "        doc = np.array(documents[m])\n",
    "        N = len(doc)\n",
    "        phi.append(np.ones([N,k]) * 1/float(k)) # uniform over topics\n",
    "        \n",
    "        for i in range(k):\n",
    "            gamma[m][i] = alpha[m][i] + N/float(k)\n",
    "        m += 1 # WHYYYYYYY?\n",
    "        \n",
    "    return alpha, beta, gamma, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lower_bound_likelihood(Phi, gamma, alpha, Beta, document, vocabulary, k):\n",
    "    likelihood = 0.0\n",
    "    V = len(vocabulary)\n",
    "    words = np.array(document)\n",
    "    N = len(words)\n",
    "    \n",
    "    alpha_sum = 0.0\n",
    "    phi_gamma_sum = 0.0\n",
    "    phi_logbeta_sum = 0.0\n",
    "    entropy_sum = 0.0\n",
    "    gamma_sum = 0.0\n",
    "    \n",
    "    alpha_sum += gammaln(np.sum(alpha))  \n",
    "    gamma_sum -= gammaln(np.sum(gamma)) \n",
    "    for i in range(k):\n",
    "        alpha_sum += -gammaln(alpha[i]) + \\\n",
    "                (alpha[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma)))\n",
    "        \n",
    "        for n in range(N):\n",
    "            if Phi[n,i] > 0:\n",
    "                w_indicator = np.sum(np.in1d(vocabulary, words[n]))   \n",
    "                phi_gamma_sum += Phi[n,i] * (digamma(gamma[i]) - digamma(np.sum(gamma[:])))\n",
    "                entropy_sum += Phi[n,i] * np.log(Phi[n,i])\n",
    "                for j in range(V):\n",
    "                    if Beta[i,j] > 0:\n",
    "                        phi_logbeta_sum += Phi[n,i] * w_indicator * np.log(Beta[i,j])\n",
    "            \n",
    "        gamma_sum += gammaln(gamma[i]) - \\\n",
    "                    (gamma[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma[:])))\n",
    "    \n",
    "    likelihood += (alpha_sum + phi_gamma_sum + phi_logbeta_sum - gamma_sum - entropy_sum) \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_phi_gamma(k, N, phi, gamma, alpha, beta, words, document, vocabulary, tol=1e-5, MAX_STEPS = 100):\n",
    "    \n",
    "    likelihood = 0.0\n",
    "    security_count = 0\n",
    "    converged = False\n",
    "\n",
    "    while (not converged) and (security_count < MAX_STEPS):\n",
    "        security_count += 1\n",
    "            \n",
    "        phi_old = phi\n",
    "        phi = np.zeros([N,k])\n",
    "        gamma_old = gamma\n",
    "            \n",
    "        for n in range(N):\n",
    "            word = words[n]\n",
    "            if len(np.where(vocabulary == word)[0]) > 0: # word does not exist in vocabulary\n",
    "                for i in range(k):                \n",
    "                    beta_ = beta[i, np.where(vocabulary == word)]\n",
    "                    phi[n, i] = beta_[0][0] * np.exp(digamma(gamma[i]) - digamma(np.sum(gamma)))\n",
    "                phi[n,:] = phi[n,:] / np.sum(phi[n,:])   \n",
    "        gamma = alpha + np.sum(phi, axis=0)    \n",
    "            \n",
    "\n",
    "        # Convergence ctierion: did phi and gamma change significantly?\n",
    "        if (np.linalg.norm(phi - phi_old) < tol) and (np.linalg.norm(gamma - gamma_old) < tol):              \n",
    "            print('Document needed ' + str(security_count) + ' iterations to converge.')\n",
    "                \n",
    "            likelihood += compute_lower_bound_likelihood(phi, gamma, alpha, beta, document, vocabulary, k)\n",
    "            converged = True\n",
    "    \n",
    "    return phi, gamma, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_beta(phi, gamma, alpha, documents, vocabulary, k, M, V):\n",
    "    beta = np.zeros([k, V])\n",
    "    for m in range(M):\n",
    "        words = np.array(documents[m])\n",
    "        phi_m = phi[m]\n",
    "        for i in range(k):\n",
    "            phi_ = phi_m[:,i]\n",
    "            for j in range(V):\n",
    "                word = vocabulary[j]\n",
    "                indicator = np.in1d(words, word)\n",
    "                indicator.astype(int) \n",
    "                beta[i][j] += np.dot(indicator, phi_)\n",
    "    beta = np.transpose(np.transpose(beta) / np.sum(beta, axis=1))\n",
    "\n",
    "    return beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha(alpha, gamma, k, M, max_iter=50, tol=0.001):\n",
    "    \"\"\"This function updates alpha.\"\"\"\n",
    "    \n",
    "    counter = 0\n",
    "    check = False # flag for convergence\n",
    "    \n",
    "    stats = 0\n",
    "    sum_gamma = np.sum(gamma, axis=1)   # (D x 1)\n",
    "    for d in range(M):\n",
    "        stats = stats - k * special.polygamma(0, sum_gamma[d]) + np.sum(special.polygamma(0, gamma[d]))\n",
    "    \n",
    "    while(not check):\n",
    "        \n",
    "        if np.isnan(alpha).any():\n",
    "            alpha = np.ones([M,k])*0.5 # re-intialize to avoid error\n",
    "        else:\n",
    "            counter += 1\n",
    "                \n",
    "            gradient = M * k * special.polygamma(0, k*alpha)\n",
    "            gradient = gradient - M * k * special.polygamma(0, alpha)\n",
    "            \n",
    "            hessian = M * k * k * special.polygamma(1, k*alpha)\n",
    "            hessian = hessian - M * k * special.polygamma(1, alpha)\n",
    "    \n",
    "            temp = gradient / (hessian * alpha + gradient + tol)\n",
    "            log_alpha = np.log(alpha) - temp\n",
    "            alpha = np.exp(log_alpha)\n",
    "            \n",
    "            if counter == max_iter:\n",
    "                check = True\n",
    "            else:\n",
    "                if counter > 1 and np.absolute(gradient).all() < tol:\n",
    "                    check = True                    \n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(phi, gamma, alpha, beta, documents, vocabulary, k, M):\n",
    "\n",
    "    for m in range(M):\n",
    "        words = np.array(documents[m])\n",
    "        N = len(words)\n",
    "        \n",
    "        phi[m], gamma[m,:], likelihood = update_phi_gamma(k, N, phi[m], gamma[m,:], alpha[m,:], beta, words, documents[m], vocabulary)\n",
    "                \n",
    "    return phi, gamma, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(phi, gamma, alpha, documents, vocabulary, k, M):\n",
    "    print('M-step')\n",
    "    V = len(vocabulary)\n",
    "    beta = update_beta(phi, gamma, alpha, documents, vocabulary, k, M, V)\n",
    "    alpha = update_alpha(alpha, gamma, k, M)\n",
    "    \n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "corpus = corpus[:5]\n",
    "M = len(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_EM(Phi_init, gamma_init, alpha_init, Beta_init, documents, vocabulary, k, M):\n",
    "    print('Variational EM')\n",
    "    \n",
    "    likelihood = 0\n",
    "    likelihood_old = 0\n",
    "    iteration = 1 # Initialization step is the first step\n",
    "    Phi = Phi_init\n",
    "    gamma = gamma_init\n",
    "    alpha = alpha_init\n",
    "    Beta = Beta_init\n",
    "    while iteration <= 2 or np.abs((likelihood-likelihood_old)/likelihood_old) > 1e-4:\n",
    "        # Update parameters \n",
    "        likelihood_old = likelihood\n",
    "        Phi_old = Phi \n",
    "        gamma_old = gamma \n",
    "        alpha_old = alpha\n",
    "        Beta_old = Beta\n",
    "    \n",
    "        Phi, gamma, likelihood = \\\n",
    "            E_step(Phi_old, gamma_old, alpha_old, Beta_old, documents, vocabulary, k, M)\n",
    "        alpha, Beta = \\\n",
    "            M_step(Phi, gamma, alpha_old, documents, vocabulary, k, M)\n",
    "                \n",
    "        iteration += 1\n",
    "        \n",
    "        if iteration > 100:\n",
    "            break\n",
    "        \n",
    "    return Phi, gamma, alpha, Beta, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_init, beta_init, gamma_init, phi_init = initialize_parameters(corpus, vocabulary, k, M)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document needed 58 iterations to converge.\n",
      "Document needed 19 iterations to converge.\n",
      "Document needed 100 iterations to converge.\n",
      "Document needed 54 iterations to converge.\n",
      "Document needed 16 iterations to converge.\n"
     ]
    }
   ],
   "source": [
    "phi, gamma, likelihood = E_step(phi, gamma, alpha, beta, corpus, vocabulary, k, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[123.0665215 ,  51.78779227,  64.72712337,  27.41856286],\n",
       "       [ 77.70209167,  62.99840727,  97.49119094,  37.80831012],\n",
       "       [ 86.72562163,  73.92728346,  30.92108954,  55.42600538],\n",
       "       [ 86.12257718,  59.6294638 ,  25.48414435,  34.76381467],\n",
       "       [ 22.84323269,   5.6151733 ,  16.65932049,  23.88227352]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M-step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bjeli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in subtract\n",
      "C:\\Users\\bjeli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in subtract\n",
      "C:\\Users\\bjeli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "alpha, beta = M_step(phi, gamma, alpha, corpus, vocabulary, k, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational EM\n",
      "M-step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bjeli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in subtract\n",
      "C:\\Users\\bjeli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in subtract\n",
      "C:\\Users\\bjeli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document needed 97 iterations to converge.\n",
      "Document needed 61 iterations to converge.\n",
      "Document needed 75 iterations to converge.\n",
      "M-step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bjeli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M-step\n",
      "M-step\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-358-40d14f93d8cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mPhi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m         \u001b[0mvariational_EM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphi_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-352-8ec4f324e7b0>\u001b[0m in \u001b[0;36mvariational_EM\u001b[1;34m(Phi_init, gamma_init, alpha_init, Beta_init, documents, vocabulary, k, M)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mBeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeta_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mwhile\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlikelihood_old\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlikelihood_old\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mlikelihood_old\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "Phi, gamma, alpha, Beta, likelihood = \\\n",
    "        variational_EM(phi_init, gamma_init, alpha_init, beta_init, corpus, vocabulary, k, M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
