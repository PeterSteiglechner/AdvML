{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "from scipy import special, misc\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import digamma, gammaln, polygamma\n",
    "import pandas as pd\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = np.genfromtxt('ap/vocab.txt',  dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_data(corpus_file, vocabulary_file, stopwords_file):\n",
    "    \"\"\"\n",
    "    Reads the corpus from the .txt file into a list of lists;\n",
    "    a list for each document which contains a list of all the \n",
    "    words as strings.\n",
    "    Input parameters:\n",
    "    corpus_file - path to the corpus file\n",
    "    vocabulary_file - path to the vocabulary file\n",
    "    stopwords_file - path to the stopwords file\n",
    "    \"\"\"\n",
    "    vocabulary = np.genfromtxt(vocabulary_file,  dtype='str')\n",
    "    special_chars = '1234567890~!@#Â£$%^&*()_+,./<>?\\|\"]}\\'[{`-'\n",
    "    corpus = []\n",
    "    \n",
    "    # read in stopwords from file into a list\n",
    "    stopwords = [] \n",
    "    with open(stopwords_file, 'r') as file:\n",
    "        stop_words = file.read().replace(',', ' ')\n",
    "        for word in stop_words.split():\n",
    "            stopwords.append(word) \n",
    "    \n",
    "    with open(corpus_file, 'r') as text:\n",
    "        doc = ''\n",
    "        new = False\n",
    "        for line in text:\n",
    "            if new: # reached a new document\n",
    "                if line.strip() != '</TEXT>': # until we reach the new doc\n",
    "                    for char in special_chars: # remove punctuation etc,\n",
    "                        line = line.replace(char, '') \n",
    "                    doc += line\n",
    "                else: # we've reached a new doc again\n",
    "                    doc = doc.lower() # all words lowercase\n",
    "                    words = np.array(doc.split())\n",
    "                    # PETER EDIT: next two lines\n",
    "                    doc = [word for word in words if (  (word not in stopwords) and (word in vocabulary)  )]\n",
    "                    corpus.append(doc)\n",
    "                    doc = ''\n",
    "            elif line.strip() == '<TEXT>': new = True\n",
    "\n",
    "    \n",
    "    return corpus, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocabulary = clean_up_data('ap/ap.txt', 'ap/vocab.txt', 'ap/stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(documents, vocabulary, k):\n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "    \n",
    "    # Initialize alpha \n",
    "    # alpha = np.ones([M,k]) * 50/k # for every document, for every topic\n",
    "    alpha = np.ones(k)*50/k\n",
    "    eta = 5/k\n",
    "    \n",
    "    Lambda = np.random.rand(k,V) * 0.5 + 0.5\n",
    "    \n",
    "    # Initialize beta\n",
    "    beta = np.zeros([k,V]) # for every topic, for every word in the vocabulary\n",
    "    for i in range(k):\n",
    "        beta[i] = np.random.uniform(0, 1, V)\n",
    "        beta[i] = beta[i] / np.sum(beta[i])\n",
    "    \n",
    "    # Initialize phi and gamma\n",
    "    phi = []\n",
    "    gamma = np.zeros([M,k]) # for every document, for every topic\n",
    "    for m in range(M):\n",
    "        doc = np.array(documents[m])\n",
    "        N = len(doc)\n",
    "        phi.append(np.ones([N,k]) * 1/float(k)) # uniform over topics\n",
    "        \n",
    "        for i in range(k):\n",
    "            gamma[m][i] = alpha[i] + N/float(k)\n",
    "        #m += 1 # WHYYYYYYY?\n",
    "        \n",
    "    return alpha, eta, beta, gamma, phi, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lower_bound_likelihood_smoothed(M, phi, gamma, alpha, eta, document, Lambda, documents, digamma_lambda, digamma_lambda_full):\n",
    "    K, V = Lambda.shape\n",
    "    likelihood = np.zeros([M,1])\n",
    "    N = len(document)\n",
    "    \n",
    "  \n",
    "    digamma_gamma = digamma(gamma) - digamma(np.sum(gamma))\n",
    "    \n",
    "    E_theta_alpha = gammaln(alpha*K) - K * gammaln(alpha) \\\n",
    "                        + (alpha-1) * np.sum(digamma_gamma)\n",
    "    E_z_theta = np.dot(np.sum(phi[:N,:], axis = 0), digamma_gamma)\n",
    "    E_w_z_beta = np.sum(digamma_lambda * phi[:N,:])\n",
    "    E_theta_gamma = gammaln(np.sum(gamma)) - np.sum(gammaln(gamma)) \\\n",
    "                    + np.dot(gamma - 1, digamma_gamma)\n",
    "    E_z_phi = np.sum(phi[:N,:] * np.log(phi[:N,:]))\n",
    "\n",
    "    likelihood = E_theta_alpha + E_z_theta + E_w_z_beta - E_theta_gamma - E_z_phi\n",
    "\n",
    "    E_beta_eta = K * (gammaln(eta * V) - V * gammaln(eta)) + (eta - 1) * np.sum(digamma_lambda_full)\n",
    "    E_beta_lambda = np.sum(gammaln(np.sum(Lambda, axis = 1)) - np.sum(gammaln(Lambda), axis = 1)[np.newaxis,:]) \\\n",
    "                    + np.sum((Lambda - 1) * digamma_lambda_full.T)\n",
    "\n",
    "    likelihood = np.sum(likelihood) + E_beta_eta - E_beta_lambda\n",
    "    \n",
    "    return(likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha(alpha, gamma, k, M, max_iter=50, tol=1e-4):\n",
    "    \n",
    "    # Maria B version\n",
    "    temp = 0\n",
    "    for d in range(M):\n",
    "        temp_1 = np.sum(special.polygamma(0, gamma[d])) - np.sum(special.polygamma(0, np.sum(gamma, axis=1)))\n",
    "    \n",
    "    gradient = M * (k * special.polygamma(1, alpha) - special.polygamma(1, k*alpha))\n",
    "    gradient = gradient + temp\n",
    "\n",
    "    hessian = M * k * (k * special.polygamma(2, k*alpha) - special.polygamma(2, alpha))\n",
    "\n",
    "    temp = gradient / (hessian * alpha + gradient + tol)\n",
    "    if (alpha == 0).any():\n",
    "        alpha += 0.005\n",
    "\n",
    "    log_alpha = np.log(alpha) - temp\n",
    "    alpha = np.exp(log_alpha)    \n",
    "        \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lambda(phi, eta, doc, vocabulary, k):\n",
    "    \n",
    "    V = len(vocabulary)\n",
    "    \n",
    "    Lambda = np.ones([k, V]) * eta\n",
    "    words = np.array(doc)\n",
    "    for i in range(k):\n",
    "        phi_ = phi[:,i]\n",
    "        for j in range(V):\n",
    "            word = vocabulary[j]\n",
    "            indicator = np.in1d(words, word)\n",
    "            indicator.astype(int)  \n",
    "            Lambda[i][j] += np.dot(indicator, phi_)\n",
    "                    \n",
    "    return Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_beta(phi, documents, vocabulary, k):\n",
    "    \n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "    \n",
    "    beta = np.zeros([k, V])\n",
    "    for m, doc in enumerate(documents):\n",
    "        words = np.array(doc)\n",
    "        phi_m = phi[m]\n",
    "        for i in range(k):\n",
    "            phi_ = phi_m[:,i]\n",
    "            for j in range(V):\n",
    "                word = vocabulary[j]\n",
    "                indicator = np.in1d(words, word)\n",
    "                indicator.astype(int) \n",
    "                beta[i][j] += np.dot(indicator, phi_)\n",
    "    beta = np.transpose(np.transpose(beta) / np.sum(beta, axis=1))\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_eta(eta, gamma, k, V, M, max_iter=50, tol=1e-4):\n",
    "\n",
    "    temp = 0\n",
    "    for d in range(M):\n",
    "        temp_1 = np.sum(special.polygamma(0, gamma[d])) - np.sum(special.polygamma(0, np.sum(gamma, axis=1)))\n",
    "    \n",
    "    gradient = V * (k * special.polygamma(1, eta) - special.polygamma(1, k*eta))\n",
    "    gradient = gradient + temp\n",
    "\n",
    "    hessian = V * k * (k * special.polygamma(2, k*eta) - special.polygamma(2, eta))\n",
    "\n",
    "    temp = gradient / (hessian * eta + gradient + tol)\n",
    "    if (eta == 0):\n",
    "        eta += 0.005\n",
    "\n",
    "    log_eta = np.log(eta) - temp\n",
    "    eta = np.exp(log_eta)    \n",
    "        \n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_phi_gamma_smoothed(M, k, phi, gamma, digamma_lambda, alpha, eta, Lambda, doc, vocabulary, documents, digamma_lambda_full, tol=1e-5, MAX_STEPS = 100):\n",
    "        likelihood = 0.0\n",
    "        iterations = 0\n",
    "        converged = False\n",
    "\n",
    "        N = len(doc)\n",
    "\n",
    "        while (not converged) and (iterations < MAX_STEPS):\n",
    "            iterations += 1\n",
    "            \n",
    "            #digamma_gamma = digamma(gamma) - digamma(np.sum(gamma))\n",
    "\n",
    "            phi_old = phi\n",
    "            phi = np.zeros([N,k])\n",
    "            gamma_old = gamma\n",
    "\n",
    "            digamma_gamma = digamma(gamma) - digamma(np.sum(gamma))\n",
    "            phi[:N,:] = digamma_gamma + digamma_lambda\n",
    "            phi[:N,:] = np.exp(phi[:N,:] - special.logsumexp(phi[:N,:], axis = 1)[:,np.newaxis])\n",
    "            \n",
    "            gamma = alpha + np.sum(phi[:N,:], axis = 0)\n",
    "\n",
    "            # Convergence ctierion: did phi and gamma change significantly?\n",
    "            if (np.linalg.norm(phi - phi_old) < tol) and (np.linalg.norm(gamma - gamma_old) < tol):              \n",
    "                print(str(iterations) + ' iterations to converge.')\n",
    "\n",
    "                likelihood += compute_lower_bound_likelihood_smoothed(M, phi, gamma, alpha, eta, doc, Lambda, documents, digamma_lambda, digamma_lambda_full)\n",
    "                converged = True\n",
    "\n",
    "        return phi, gamma, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step_smoothed(M, phi, gamma, alpha, eta, Lambda, documents, vocabulary, k):\n",
    "    print('E-step')\n",
    "    \n",
    "    digamma_lambda = digamma(Lambda.T) - digamma(np.sum(Lambda, axis = 1))\n",
    "    for d, doc in enumerate(documents):\n",
    "        phi[d], gamma[d], likelihood = update_phi_gamma_smoothed(M, k, phi[d], gamma[d], digamma_lambda[d], alpha, eta, Lambda, doc, vocabulary, Lambda, digamma_lambda)\n",
    "        Lambda[d] = update_lambda(phi[d], eta, doc, vocabulary, k)\n",
    "        \n",
    "    return phi, gamma, Lambda, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step_smoothed(phi, gamma, alpha, eta, documents, vocabulary, k):\n",
    "    print('M-step')\n",
    "    \n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    alpha = update_alpha(alpha, gamma, k, M)\n",
    "    eta = update_eta(eta, gamma, k, V, M)\n",
    "    \n",
    "    return alpha, eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_EM_smoothed(M, phi_init, gamma_init, alpha_init, beta_init, Lambda_init, eta_init, documents, vocabulary, k, tol=1e-5):\n",
    "    print('Variational EM')\n",
    "    \n",
    "    M = len(documents)\n",
    "    \n",
    "    likelihood = 0\n",
    "    likelihood_old = 0.000004\n",
    "    \n",
    "    iteration = 1 # Initialization step is the first step\n",
    "    \n",
    "    phi = phi_init\n",
    "    gamma = gamma_init\n",
    "    alpha = alpha_init\n",
    "    beta = beta_init\n",
    "    Lambda = Lambda_init\n",
    "    eta = eta_init\n",
    "    \n",
    "    converged = False\n",
    "    \n",
    "    while (not converged):\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Update parameters \n",
    "        if likelihood == 0:\n",
    "            print(\"Likelihood==0\")\n",
    "            likelihood_old = 0.005\n",
    "        else:\n",
    "            likelihood_old = likelihood\n",
    "        phi_old = phi \n",
    "        gamma_old = gamma \n",
    "        alpha_old = alpha\n",
    "        beta_old = beta\n",
    "        Lambda_old = Lambda\n",
    "        eta_old = eta\n",
    "        \n",
    "    \n",
    "        phi, gamma, Lambda, likelihood = \\\n",
    "            E_step_smoothed(M, phi_old, gamma_old, alpha_old, eta_old, Lambda_old, documents, vocabulary, k)\n",
    "        alpha, eta = \\\n",
    "            M_step_smoothed(phi, gamma, alpha, eta, documents, vocabulary, k)\n",
    "                \n",
    "        if iteration > 15:\n",
    "            break\n",
    "        \n",
    "        # check convergence\n",
    "        if (np.abs((likelihood - likelihood_old) / likelihood_old) > tol):\n",
    "            if (iteration > 2):\n",
    "                converged = True\n",
    "        \n",
    "    return phi, gamma, Lambda, alpha, eta, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main: smoothed LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "corpus_reduced = corpus[:3]\n",
    "M = len(corpus_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_init, eta_init, beta_init, gamma_init, phi_init, Lambda_init = initialize_parameters(corpus_reduced, vocabulary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational EM\n",
      "Likelihood==0\n",
      "E-step\n",
      "82 iterations to converge.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (3,10473) into shape (10473)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-863ba38b0841>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvariational_EM_smoothed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_reduced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-b541d88bb012>\u001b[0m in \u001b[0;36mvariational_EM_smoothed\u001b[1;34m(M, phi_init, gamma_init, alpha_init, beta_init, Lambda_init, eta_init, documents, vocabulary, k, tol)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mE_step_smoothed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mM_step_smoothed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-bfcda5ca158a>\u001b[0m in \u001b[0;36mE_step_smoothed\u001b[1;34m(M, phi, gamma, alpha, eta, Lambda, documents, vocabulary, k)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mphi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_phi_gamma_smoothed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdigamma_lambda\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdigamma_lambda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mLambda\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_lambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (3,10473) into shape (10473)"
     ]
    }
   ],
   "source": [
    "phi, gamma, Lambda, alpha, eta, likelihood = \\\n",
    "variational_EM_smoothed(M, phi_init, gamma_init, alpha_init, beta_init, Lambda_init, eta_init, corpus_reduced, vocabulary, k, tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
