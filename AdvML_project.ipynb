{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "be_iIWK5cDKa"
   },
   "source": [
    "# Project Adv ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-IscMsTcDKh"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "from scipy import special\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASugsoSOcDKx"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFfdzosccDK0"
   },
   "outputs": [],
   "source": [
    "##### PARAMS ####\n",
    "# k = number of topics\n",
    "# N = Number of Words in a Document (different for each doc)\n",
    "# M = Number of Docs\n",
    "# z_n = [k]-vector = topic distribution for word n \n",
    "# Theta = [k]-dim vector = mixture weights\n",
    "\n",
    "# Gamma = k-dim vector = Determines Theta in Variational Model\n",
    "# Phi = phi_1 .. phi_N = [N x k] Matrix = it determines the probability distribution for topics z of words in Variational Model\n",
    "\n",
    "# alpha = [k] - vector = prior probability for theta (mixture weights) (alpha>0)\n",
    "# beta = [k x V] - Matrix = with beta_ij = p( w^j = 1 | z^i = 1) = prob of for a specific word j given a specific topic i\n",
    "\n",
    "# D = list of [VxN]-dim matrices, that is M long = [\\mathbf{w}_1, ... \\mathbf{w}_M], where \\mathbf{w}=[w_1,...,w_N] is VxN (one document consisting of N words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QRR-82W_cDK-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqevQ4p3c-kp"
   },
   "source": [
    "## Load the Document Corpus\n",
    "Download the data from \n",
    "https://github.com/Blei-Lab/lda-c/blob/master/example/ap.tgz\n",
    "\n",
    "We can directly load the file \"ap.dat\" which contains:\n",
    "\n",
    "1 line = 1 document,\n",
    "\n",
    "[number of different words in doc] [word index (where the one is in w_n]:[how often it occurs in the doc] [word index 2]:[occurences 2] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Ea5B3x4Kxqj"
   },
   "outputs": [],
   "source": [
    "V = 10473 # Vocab Size given by vocab_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rox27_k9o8A"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('ap/ap.dat', sep=\"#\", names=['A'])\n",
    "data = data.A.str.split(' |:', n=V, expand=True)\n",
    "data_np = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r_A7n5dz_svz"
   },
   "outputs": [],
   "source": [
    "M = data_np.shape[0] # number of documents\n",
    "#V = data_np.shape[1] # vocab list # actually this should be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNvZhGEhAw82"
   },
   "outputs": [],
   "source": [
    "D = []  # this has dim: M x 2 (words and counts) x N_words(document)\n",
    "for w in range(M): # for each document\n",
    "    doc_string = data_np[w,:]\n",
    "    #print(len(doc_string))\n",
    "    word_indices = doc_string[1::2]\n",
    "    #print(\"word_indices\", word_indices)\n",
    "    counts = doc_string[2::2]\n",
    "    #if w==1:\n",
    "    #  print(len(word_indices), len(counts))\n",
    "    #print(\"counts\", counts)\n",
    "    N_different_words_in_this_doc = doc_string[0] # not needed.\n",
    "    D.append([])\n",
    "    all_words = []\n",
    "    for n,word in enumerate(word_indices):\n",
    "        if word==None:\n",
    "            break\n",
    "        for repetition in range(int(counts[n])):\n",
    "            #if int(counts[n])>1:\n",
    "            #  print(word, counts[n])\n",
    "            all_words.append(int(word))\n",
    "        D[-1].append(all_words)\n",
    "    # NOTE: D does not contain M docs with each N_d words, where each word w_n is a V-dim vector.\n",
    "  # BUT:              it has M docs with each N_d words, where each word is just the unique index v, that is one of the V-dim vector.\n",
    "  # Reason: V=10,000...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Y3g6rEwdTCL"
   },
   "source": [
    "## Remove the standard words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKLClHU2dWwO"
   },
   "outputs": [],
   "source": [
    "# I think this is already done in the \"ap.dat\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1qxBd2fcDLG"
   },
   "source": [
    "### Variational Inference (the E-Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k8BL3H0scDLJ"
   },
   "outputs": [],
   "source": [
    "def updateGammaPhi(k, alpha, beta, w, tol=10**(-5), MAX_STEPS=100):\n",
    "    # This implements the update equations for Gamma and Phi for a variational inference step.\n",
    "    # Inputs: k=int=nr of topics;  alpha=[k]-vec=priors_of_theta(mixture_weights);  beta=[NxV]-matrix=prob_of_word_v_given_topic_\n",
    "    # Init\n",
    "    N = len(w)\n",
    "    #print(np.shape(alpha), np.shape(beta), k, np.shape(D))\n",
    "    phi = 1/k * np.ones([N,k])\n",
    "    gamma = alpha + np.ones([k]) * N/k \n",
    "    q_old = compute_lower_bound_likelihood(w,phi,gamma, alpha, beta)\n",
    "    converged = False\n",
    "    #Loop\n",
    "    security_count=0\n",
    "    while not converged and security_count < MAX_STEPS:\n",
    "        security_count+=1\n",
    "        phi_old = np.copy(phi)\n",
    "        gamma_old = np.copy(gamma)\n",
    "        for n in range(N):\n",
    "            for i in range(k):\n",
    "                w_n = w[n] ## this is not a V-Vector, but just the index of the word in the vocab\n",
    "                #beta_iv is p(w_n^v = 1 | z^i = 1) \n",
    "                v = w_n   #unique v for each word w_n\n",
    "                phi[n,i] = beta[i,v] * np.exp(psi(gamma_old[i])- psi(np.sum(gamma_old)))\n",
    "                # NOTE: In .c file, they use: phi[n,i] = Psi(gamma[i]) + log_prob_w[i][v], then (log)-sum??, then take exp(phi[n,k] - logsum(phi))\n",
    "            #normalize Phi\n",
    "            phi[n,:] = phi[n,:]/np.sum(phi[n,:])\n",
    "        for i in range(k):\n",
    "            gamma[i] = alpha[i] + np.sum(phi[:,i])\n",
    "        # in .c file: they also use a very different formula.\n",
    "        \n",
    "        # Convergence criterion is: lower bound of likleihood\n",
    "        q_new = compute_lower_bound_likelihood(w,phi,gamma, alpha, beta)\n",
    "        if abs(q_new-q_old)<tol:\n",
    "            converged = True\n",
    "        else: \n",
    "            q_old=np.copy(q_new)\n",
    "    return q_new, gamma, phi\n",
    "\n",
    "def psi(gamma_i):\n",
    "    # this is the first derivative (via Taylor approximation) of the log \\Gamma function\n",
    "    # according to Wikipedia this is the \"digamma\" function\n",
    "    return scipy.special.digamma(gamma_i)\n",
    "\n",
    "def lgamma(gamma_i):\n",
    "    return scipy.special.loggamma(gamma_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 785,
     "status": "ok",
     "timestamp": 1578574121102,
     "user": {
      "displayName": "Peter Steiglechner",
      "photoUrl": "",
      "userId": "08437803724975041227"
     },
     "user_tz": -60
    },
    "id": "Da6xqGirQCww",
    "outputId": "46cc7b62-411b-481c-865e-cfae9bf340f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USE OF LAMBDA\n",
    "a=np.ones(19)\n",
    "f = lambda i: a[i]\n",
    "f(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhAPHiqOMHOn"
   },
   "outputs": [],
   "source": [
    "def compute_lower_bound_likelihood(w,phi,gamma, alpha, beta, verbose=False):\n",
    "    # This calculate the lower bound of L(gamma, phi, alpha, beta)\n",
    "    N = len(w)\n",
    "    global V\n",
    "    # This corresponds to equation 15 in the paper in Appendix 3.\n",
    "    loggamma_sum = lambda x: np.log(scipy.special.gamma(np.sum(x)))\n",
    "    loggamma_x_i = lambda x, i: np.log(scipy.special.gamma(x[i]))\n",
    "    E_log_thetai_givenGamma = lambda i:  (psi(gamma[i]) - psi(np.sum(gamma))) \n",
    "\n",
    "    L = loggamma_sum(alpha) - loggamma_sum(gamma)\n",
    "    if verbose: print(\"#0: \", L, end=\", \")\n",
    "    for i in range(k):\n",
    "        L += -loggamma_x_i(alpha,i) + (alpha[i]-1)*E_log_thetai_givenGamma(i)\n",
    "        L += +loggamma_x_i(gamma,i) - (gamma[i]-1)*E_log_thetai_givenGamma(i)\n",
    "        if verbose: print(\"#\",i,\": \", L, end=\", \")\n",
    "        for n in range(N):\n",
    "            L+= phi[n,i] * E_log_thetai_givenGamma(i)\n",
    "            L+= - phi[n,i] *np.log(phi[n,i])\n",
    "            #for j in range(V):\n",
    "            #  L+= phi[n,i]*... easier below, \n",
    "            # since sum over j=1..V w_n^j gives only one contribution (unique v for word w_n\n",
    "            v = w[n] # here w_n is not a vector\n",
    "            L+= phi[n,i] * np.log(beta[i,v]) \n",
    "            if verbose: print(\"#\",i,\",\",n,\": \", L, \", v=\",v, end=\", \")\n",
    " \n",
    "    if verbose: print(L)\n",
    "    return L\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M1MHz8jCcDLa"
   },
   "outputs": [],
   "source": [
    "def E_step(k, alpha,beta, MAX_COUNTER=1000):\n",
    "    gamma_list = []\n",
    "    phi_list = []\n",
    "    counter=0\n",
    "    likelihood_list = []\n",
    "    for doc in D:\n",
    "        counter+=1\n",
    "        # doc is a N_d Vector containing indices of all words\n",
    "        #print(\"DOCUMENT, \", len(doc), len(doc[0]))\n",
    "        likelihood, gamma, phi = updateGammaPhi(k,alpha,beta,doc[0])\n",
    "        gamma_list.append(gamma)\n",
    "        phi_list.append(phi)\n",
    "        likelihood_list.append(likelihood)\n",
    "        # Stop this if sufficient statistics (how do i know this?)\n",
    "        if counter%10 ==0:\n",
    "            print(gamma, phi, likelihood)\n",
    "        # calculate the approximate q(theta, z | gamma, phi)\n",
    "        if counter> MAX_COUNTER:\n",
    "            break\n",
    "    return likelihood_list, gamma_list, phi_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bwKHt5Lkbnu"
   },
   "source": [
    "## M-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 132
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 833,
     "status": "error",
     "timestamp": 1578575254498,
     "user": {
      "displayName": "Peter Steiglechner",
      "photoUrl": "",
      "userId": "08437803724975041227"
     },
     "user_tz": -60
    },
    "id": "QpOodnOHkcyS",
    "outputId": "9e69987e-967c-4706-c5f8-8df27ebe63a8"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-34-bc2016d8a2a2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-bc2016d8a2a2>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def M_step():\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XlMgdbYiKmZB"
   },
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wT8WK33FKleu"
   },
   "outputs": [],
   "source": [
    "k= 10\n",
    "V= 10473\n",
    "\n",
    "alpha=np.ones([k])/k\n",
    "\n",
    "beta = np.zeros([k,V])+0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1SNSQlB7KlY4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "likelihood_list, gamma_list, phi_list = E_step(k,alpha,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1578574395942,
     "user": {
      "displayName": "Peter Steiglechner",
      "photoUrl": "",
      "userId": "08437803724975041227"
     },
     "user_tz": -60
    },
    "id": "m3Mvnv9tKlTp",
    "outputId": "b94ced5a-5811-4e7c-f75a-5116ffdaadb1"
   },
   "outputs": [],
   "source": [
    "print(likelihood_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NAtlSQmNKlGQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AdvML_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
