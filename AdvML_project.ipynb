{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "be_iIWK5cDKa"
   },
   "source": [
    "# Project Adv ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-IscMsTcDKh"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "from scipy import special\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASugsoSOcDKx"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFfdzosccDK0"
   },
   "source": [
    "PARAMETERS:\n",
    "\n",
    "k - number of topics\n",
    "N - number of words in a document (different for each document)\n",
    "M - number of documents in a corpus\n",
    "\n",
    "Model parameters:\n",
    "z_n - [k] dimension vector; topic distribution for word n \n",
    "Theta - [k] dimension vector; mixture weights\n",
    "alpha - [k] dimension vector; prior probability for theta (mixture weights) (alpha > 0)\n",
    "beta - [k x V] dimension matrix; beta_ij = p(w^j = 1 | z^i = 1) \n",
    "                                 probability for a specific word j given a specific topic i\n",
    "D - list of [V x N] dimension matrices, that is M long = [\\mathbf{w}_1, ... \\mathbf{w}_M];\n",
    "                                 where \\mathbf{w} = [w_1,...,w_N] is [V x N] (one document consisting of N words) \n",
    "\n",
    "\n",
    "Variational parameters:\n",
    "Gamma - [k] dimension vector; determines Theta in the Variational Model\n",
    "Phi = phi_1 .. phi_N - [N x k] dimension matrix; determines the probability distribution \n",
    "                                 for topics z of words in the Variational Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqevQ4p3c-kp"
   },
   "source": [
    "## Load the Document Corpus\n",
    "Download the data from \n",
    "https://github.com/Blei-Lab/lda-c/blob/master/example/ap.tgz\n",
    "\n",
    "We can directly load the file \"ap.dat\" which contains:\n",
    "\n",
    "1 line = 1 document,\n",
    "\n",
    "[number of different words in doc] [word index (where the one is in w_n]:[how often it occurs in the doc] [word index 2]:[occurences 2] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Ea5B3x4Kxqj"
   },
   "outputs": [],
   "source": [
    "vocab = pd.read_csv(\"./ap/vocab.txt\", header=None)\n",
    "V = vocab.shape[0] # Vocabulary size given by vocab_list.txt\n",
    "#vocab.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rox27_k9o8A"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('ap/ap.dat', sep=\"#\", names=['A'])\n",
    "M = data.shape[0]\n",
    "data = data.A.str.split(' ')\n",
    "doc_word_count = np.zeros((M, V))\n",
    "\n",
    "for j, m in enumerate(data):\n",
    "    count = m.pop(0)\n",
    "    for i in range(int(count)):\n",
    "        type(m[i])\n",
    "        cur = m[i].split(\":\")\n",
    "        doc_word_count[j][int(cur[0])] = int(cur[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rox27_k9o8A"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('ap/ap.dat', sep=\"#\", names=['A'])\n",
    "data = data.A.str.split(' |:', n=V, expand=True)\n",
    "data.head()\n",
    "data_np = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNvZhGEhAw82"
   },
   "outputs": [],
   "source": [
    "D = []  # this has dim: M x 2 (words and counts) x N_words(document)\n",
    "for w in range(M): # for each document\n",
    "    doc_string = data_np[w,:]\n",
    "    #print(len(doc_string))\n",
    "    word_indices = doc_string[1::2]\n",
    "    #print(\"word_indices\", word_indices)\n",
    "    counts = doc_string[2::2]\n",
    "    #if w==1:\n",
    "    #  print(len(word_indices), len(counts))\n",
    "    #print(\"counts\", counts)\n",
    "    N_different_words_in_this_doc = doc_string[0] # not needed.\n",
    "    D.append([])\n",
    "    all_words = []\n",
    "    for n,word in enumerate(word_indices):\n",
    "        if word == None:\n",
    "            break\n",
    "        for repetition in range(int(counts[n])):\n",
    "            #if int(counts[n])>1:\n",
    "            #  print(word, counts[n])\n",
    "            all_words.append(int(word))\n",
    "    D[-1] = all_words\n",
    "  # NOTE: D does not contain M docs with each N_d words, where each word w_n is a V-dim vector.\n",
    "  # BUT:              it has M docs with each N_d words, where each word is just the unique index v, that is one of the V-dim vector.\n",
    "  # Reason: V=10,000...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246, 10473)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'count')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = []\n",
    "for i in range(len(D)):\n",
    "    lengths.append(len(D[i]))\n",
    "p = plt.hist(lengths, bins=40)\n",
    "plt.xlabel(\"Length of a document\")\n",
    "plt.ylabel(\"count\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Y3g6rEwdTCL"
   },
   "source": [
    "## Remove the standard words and load new Dataset (Reuters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the Natural Langauge Processing Package\n",
    "This provides the Reuters articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the reuters data, if not already downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# If you run this cell for the first time, uncomment the following line to download the corpus\n",
    "#nltk.download('reuters')\n",
    "#reuters.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable reuters consists of 10788 documents, with a few labeled as test and 7769 as training documents.\n",
    "\n",
    "Below, we get the filenames of the training ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_max_reuters =  7769\n"
     ]
    }
   ],
   "source": [
    "training_ids = []\n",
    "test_ids = []\n",
    "for name in reuters.fileids():\n",
    "    if \"training\" in name:\n",
    "        training_ids.append(name)\n",
    "    else:\n",
    "        test_ids.append(name)\n",
    "print(\"M_max_reuters = \", len(training_ids) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more documents than in the AP example above. Hence, we choose only those 2246 documents (for now) and compare it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_reuters = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training = includes a list of all the chosen documents. Each of these elements contains a list of all the words in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  20  Documents with sizes:  [633, 259, 119, 155, 115, 34, 250, 83, 182, 133, 126, 85, 84, 111, 99, 101, 67, 26, 73, 140]\n"
     ]
    }
   ],
   "source": [
    "training=[]  \n",
    "for i in range(M_reuters):\n",
    "    #file=str(i+1)\n",
    "    file = training_ids[i] # filename\n",
    "    training.append(reuters.words(file)) #append the document to training\n",
    "\n",
    "doc_lengths =  [len(training[i]) for i in range(len(training))] # this is N_d (length of each of the documents)\n",
    "print(\"We have \", len(training), \" Documents with sizes: \", doc_lengths[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually create the dataset and vocab_list\n",
    "\n",
    "Example:  \n",
    "\n",
    "a sentence/document: \"You have a cactus.\" \n",
    "\n",
    "    - want to remove the \"you\", the \"a\" and the \".\" (stopwords) \n",
    "    - add the rest to my vocab_list ie. vocab_list= [\"have\", \"cactus\"].\n",
    "    - Then, store the document as a list of indices from my vocab: Ie. the document above is stored as [0,1].\n",
    "- the Vocab is a list of all the (non-unnecessary) words in the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST\\ndata_test = [\"All work and no play makes 4 jack dull boy.\", \\n        \"All people make mistakes, but 4 5 jack makes no work.\",\\n        \"People are dull, jack makes 4.5 mistakes\"]\\n\\nD = create_dict(data_test)\\nD, vocab_list\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list=[] # \n",
    "\n",
    "def create_dict(data, test = True):     \n",
    "    global vocab_list # change the global vocab_list\n",
    "    D=[]  # dim = (#documents, #N_reduced_d),    where N_reduced_d is the Nr of words that are not filtered / are not unnecessary\n",
    "    for d in data: # for each document\n",
    "        # Uncomment 3 lines below to test the function with the string below \n",
    "        # test=True\n",
    "        if test==False: \n",
    "           d = \" \".join(d)\n",
    "        d = \" \".join(d)\n",
    "        dclean = remove_unnecessary_words(d)   # list of all the non-unnecessary words of the document\n",
    "        d_vocabInd = []  # This is the document in terms of the indices of the words in the vocab_list (example above: [0,1])\n",
    "        for w in dclean:  # loop through these words\n",
    "            w = w.lower() \n",
    "            if w in vocab_list: \n",
    "                pass # word is already in vocab_list\n",
    "            else:\n",
    "                #add word to vocab_list\n",
    "                vocab_list.append(w)\n",
    "            \n",
    "            # Store the word as the document\n",
    "            v = vocab_list.index(w) #index v\n",
    "            d_vocabInd.append(v)\n",
    "            \n",
    "        D.append(d_vocabInd)   # Add document to corpus\n",
    "    return D\n",
    "\n",
    "\n",
    "def remove_unnecessary_words(document):\n",
    "    words = word_tokenize(document)\n",
    "    wordsFiltered = []\n",
    "    for w in words:\n",
    "        if conditions(w):  # all the conditions what define an unnecessary word\n",
    "            # word is not unnecessary --> add to cleaned document\n",
    "            wordsFiltered.append(w)\n",
    "            \n",
    "    return wordsFiltered\n",
    "\n",
    "\n",
    "def conditions(word):\n",
    "    '''Return True if all conditions are fulfilled'''\n",
    "    Cond=True\n",
    "    # Word is no stopword\n",
    "    Cond *= (word not in stopwords.words('english'))\n",
    "    # Word is no special sign (like \".\")\n",
    "    Cond *= (word not in ['.', ',', ';', '-', '+', \\\n",
    "                             '?', '!', '=', '(', ')', \\\n",
    "                            '/', '&', '$', '€'])\n",
    "    # Word is no float (\"4.2\")\n",
    "    if (\".\" in word) or (\",\" in word):\n",
    "        if word[0].isdecimal():\n",
    "            Cond=False\n",
    "    # Word is no digit\n",
    "    Cond *= (not word.isdigit())\n",
    "    Cond *= (not word.isdecimal())\n",
    "    \n",
    "    return Cond\n",
    "\n",
    "'''TEST\n",
    "data_test = [\"All work and no play makes 4 jack dull boy.\", \n",
    "        \"All people make mistakes, but 4 5 jack makes no work.\",\n",
    "        \"People are dull, jack makes 4.5 mistakes\"]\n",
    "\n",
    "D = create_dict(data_test)\n",
    "D, vocab_list\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually do the pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [] # init vocab_list\n",
    "D_reuters = create_dict(training)\n",
    "\n",
    "#print(\"Rows = Document[1..M] (first 100 docs only), Column = Word_in_Doc[1...N_doc], data = Index in Vocab\")\n",
    "#print(pd.DataFrame(D_reuters[:100], dtype=int ))\n",
    "#print(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following: Just analysis of the Reuters data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V  660 , M  20\n"
     ]
    }
   ],
   "source": [
    "V_reuters = len(vocab_list)\n",
    "M_reuters = len(D_reuters)\n",
    "print(\"V \",V_reuters, \", M \", M_reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.75 77.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Reuters')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = [] # Lengths of the cleaned documents \n",
    "for i in range(len(D_reuters)):\n",
    "    lengths.append(len(D_reuters[i]))\n",
    "p = plt.hist(lengths, bins=40)\n",
    "p = plt.hist(doc_lengths, bins=40, alpha=0.5) # document lengths (uncleaned)\n",
    "\n",
    "print(np.mean(doc_lengths), np.mean(lengths))\n",
    "\n",
    "plt.xlabel(\"Length of a document\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Reuters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How many words did the cleaning remove?\n",
    "removed_words = np.array(doc_lengths)-np.array(lengths)\n",
    "bla = plt.hist(removed_words, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage: 0.75\n",
      "66.5 68.2601640783261\n"
     ]
    }
   ],
   "source": [
    "#Percentage of docs in which we removed from 20 to 100 words, and mean\n",
    "print(\"Percentage:\", np.sum(((removed_words<=100)*(removed_words>=20)))/removed_words.shape[0])\n",
    "print(np.mean(removed_words), np.std(removed_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: \n",
      "\n",
      " D AP:\n",
      "[0, 0, 0, 0, 0, 0, 256, 256, 256, 3, 3, 4, 4, 4, 4]    Note: if the same word appears more often in the doc, it appears in direct sequence. Hence, the same index might appear multiple times in sequence\n",
      "\n",
      " D Reuters:\n",
      "[158, 159, 160, 161, 162, 163, 164, 165, 158, 159, 160, 166, 24, 167, 165]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example: \")\n",
    "print(\"\\n D AP:\")\n",
    "print(D[100][:15], \"   Note: if the same word appears more often in the doc, it appears in direct sequence. Hence, the same index might appear multiple times in sequence\")\n",
    "print(\"\\n D Reuters:\")\n",
    "print(D_reuters[1][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1qxBd2fcDLG"
   },
   "source": [
    "### Variational Inference (the E-Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k8BL3H0scDLJ"
   },
   "outputs": [],
   "source": [
    "def updateGammaPhi(k, alpha, beta, w, tol=1e-5, MAX_STEPS=100, verbose=False):\n",
    "    ''' This implements the update equations for Gamma and Phi for a variational inference step.\n",
    "     Inputs: \n",
    "        k=int=number of topics;  \n",
    "        alpha=[k]-vec=priors_of_theta(mixture_weights);  \n",
    "        beta=[NxV]-matrix=prob_of_word_v_given_topic_\n",
    "        w=word\n",
    "    '''\n",
    "    N = len(w)\n",
    "    # Init    \n",
    "    phi = 1/k * np.ones([N,k])\n",
    "    gamma = alpha + np.ones([k]) * N/k \n",
    "    \n",
    "    q_old = compute_lower_bound_likelihood(w, k, phi, gamma, alpha, beta, verbose=verbose)\n",
    "    converged = False\n",
    "    \n",
    "    #Loop\n",
    "    security_count=0\n",
    "    \n",
    "    while (not converged) and (security_count < MAX_STEPS):\n",
    "        security_count+=1\n",
    "        \n",
    "        phi_old = np.copy(phi)\n",
    "        logphi = np.zeros_like(phi)\n",
    "        gamma_old = np.copy(gamma)\n",
    "        \n",
    "        for n in range(N):\n",
    "            for i in range(k):\n",
    "                v = w[n] ## this is not a V-Vector, but just the index of the word in the vocab\n",
    "                #beta_iv is p(w_n^v = 1 | z^i = 1) \n",
    "                #unique v for each word w_n\n",
    "                \n",
    "                ### TRICK: instead of phi=beta+e^{psi(gamma)}, do \n",
    "                logphi[n,i] = np.log(beta[i,v]) + (psi(gamma_old[i]))#\n",
    "                phi[n,i] = np.exp(logphi[n,i])\n",
    "            #normalize Phi: s.t. for each topic the sum of phi[n,:] is 1.\n",
    "            phi[n,:] = phi[n,:]/np.sum(phi[n,:])\n",
    "        gamma = alpha + np.sum(phi,axis=0)\n",
    "       \n",
    "        # Convergence criterion is: lower bound of likleihood\n",
    "        q_new = compute_lower_bound_likelihood(w, k, phi, gamma, alpha, beta, verbose=verbose)\n",
    "        #print(\"Liks, \", q_new, q_old)#,  compute_lda_lhood(w,phi,gamma,alpha,beta,k))\n",
    "        if abs(q_new-q_old) < tol:\n",
    "            converged = True\n",
    "        else: \n",
    "            q_old=np.copy(q_new)\n",
    "        \n",
    "    return q_new, gamma, phi\n",
    "\n",
    "def psi(gamma_i):\n",
    "    # this is the first derivative (via Taylor approximation) of the log \\Gamma function\n",
    "    # according to Wikipedia this is the \"digamma\" function\n",
    "    return scipy.special.digamma(gamma_i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhAPHiqOMHOn"
   },
   "outputs": [],
   "source": [
    "def compute_lower_bound_likelihood(w, k, phi, gamma, alpha, beta, verbose=False):\n",
    "    '''\n",
    "    This calculate the lower bound of L(gamma, phi, alpha, beta)\n",
    "    Ie. equation 15 in the paper in Appendix 3.\n",
    "    Only one \n",
    "    '''\n",
    "    \n",
    "    N = len(w)\n",
    "    global V # length of the vocab\n",
    "    \n",
    "    loggamma_sum = lambda x: scipy.special.gammaln(np.sum(x))\n",
    "    loggamma_x_i = lambda x, i: np.log(scipy.special.gamma(x[i]))\n",
    "    E_log_thetai_givenGamma = lambda i:  (psi(gamma[i]) - psi(np.sum(gamma))) \n",
    "\n",
    "    #print(\"gamma, alpha= \", gamma,alpha)\n",
    "    L = loggamma_sum(alpha) - loggamma_sum(gamma)\n",
    "    #if verbose: print(\"#0: \", L, end=\", \")\n",
    "    for i in range(k):\n",
    "        L += -loggamma_x_i(alpha,i) + (alpha[i]-1)*E_log_thetai_givenGamma(i)\n",
    "        L += +loggamma_x_i(gamma,i) - (gamma[i]-1)*E_log_thetai_givenGamma(i)\n",
    "        #if verbose: print(\"#\",i,\": \", L, end=\", \")\n",
    "        for n in range(N):\n",
    "            L+= phi[n,i] * E_log_thetai_givenGamma(i)\n",
    "            L+= - phi[n,i] *np.log(phi[n,i])\n",
    "            v = w[n] # here w_n is not a vector\n",
    "            L+= phi[n,i] * np.log(beta[i,v]) \n",
    "            #if verbose: print(\"#\",i,\",\",n,\": \", L, \", v=\",v, end=\", \")\n",
    "    if verbose: print(L, \"\\n\")\n",
    "    return L\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M1MHz8jCcDLa"
   },
   "outputs": [],
   "source": [
    "def E_step(D, k, alpha, beta, MAX_COUNTER=10, verbose=False):\n",
    "    gamma_list = []\n",
    "    phi_list = []\n",
    "    likelihood_list = []\n",
    "    counter = 0\n",
    "    \n",
    "    for d, doc in enumerate(D):\n",
    "        counter += 1\n",
    "        # doc is a N_d Vector containing indices of all words\n",
    "        if verbose: print(\"Doc: \"+str(d)+\" (\"+str(len(doc))+\")\", end=\", \")#, len(doc[0]))\n",
    "            \n",
    "        likelihood, gamma, phi = updateGammaPhi(k, alpha, beta, doc, verbose=verbose)\n",
    "        gamma_list.append(gamma)\n",
    "        phi_list.append(phi)\n",
    "        likelihood_list.append(likelihood)\n",
    "        \n",
    "        # Stop this if sufficient statistics (how do i know this?)\n",
    "        if verbose and counter%10 == 0:\n",
    "            print(gamma, likelihood)\n",
    "        # calculate the approximate q(theta, z | gamma, phi)\n",
    "#         if counter> MAX_COUNTER:\n",
    "#             break\n",
    "    return likelihood_list, gamma_list, phi_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bwKHt5Lkbnu"
   },
   "source": [
    "## M-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha(alpha, gamma, max_iter=50, tol=0.001):\n",
    "    \"\"\"This function updates alpha.\"\"\"\n",
    "    \n",
    "    d = len(gamma)\n",
    "    k = len(gamma[0])\n",
    "    \n",
    "    counter = 0\n",
    "    check = False # flag for convergence\n",
    "    \n",
    "    while(not check):\n",
    "        \n",
    "        if np.isnan(alpha).any():\n",
    "            alpha = 5 # re-intialize to avoid error\n",
    "        else:\n",
    "            counter += 1\n",
    "                \n",
    "            gradient = d * k * special.polygamma(0, k*alpha)\n",
    "            gradient = gradient - d * k * special.polygamma(0, alpha)\n",
    "            \n",
    "            hessian = d * k * k * special.polygamma(1, k*alpha)\n",
    "            hessian = hessian - d * k * special.polygamma(1, alpha)\n",
    "    \n",
    "            temp = gradient / (hessian * alpha + gradient)\n",
    "            log_alpha = np.log(alpha) - temp\n",
    "            alpha = np.exp(log_alpha)\n",
    "            \n",
    "            if counter == max_iter:\n",
    "                check = True\n",
    "            else:\n",
    "                if counter > 1 and np.absolute(gradient).all() < tol:\n",
    "                    check = True                    \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 132
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 833,
     "status": "error",
     "timestamp": 1578575254498,
     "user": {
      "displayName": "Peter Steiglechner",
      "photoUrl": "",
      "userId": "08437803724975041227"
     },
     "user_tz": -60
    },
    "id": "QpOodnOHkcyS",
    "outputId": "9e69987e-967c-4706-c5f8-8df27ebe63a8"
   },
   "outputs": [],
   "source": [
    "def M_step(data, k, phi, gamma, alpha, vocabulary,):\n",
    "    V = len(vocabulary)\n",
    "    M = len(data)\n",
    "    # Update beta\n",
    "    beta = np.zeros([k,V])\n",
    "    for m in range(M):\n",
    "        words = np.array(data[m])\n",
    "        phi_m = phi[m]\n",
    "    \n",
    "        for i in range(k):\n",
    "            phi_sliced = phi_m[:,i]\n",
    "            for j in range(V):\n",
    "                word = vocabulary[j]\n",
    "                bool_indicator = np.in1d(words, word) \n",
    "                beta[i,j] += np.dot(bool_indicator.astype(int), phi_sliced)\n",
    "    beta = np.transpose(np.transpose(beta) / np.sum(beta, axis=1))\n",
    "    \n",
    "    # Update alpha\n",
    "    alpha = update_alpha(alpha, gamma)\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XlMgdbYiKmZB"
   },
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wT8WK33FKleu"
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "V = 10473\n",
    "\n",
    "alpha = np.ones([k])*0.01\n",
    "beta = np.zeros([k,V])+0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D_reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1SNSQlB7KlY4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "likelihood_list, gamma_list, phi_list = \\\n",
    "    E_step(D_reuters, k, alpha, beta, MAX_COUNTER=100, verbose=False) # MAX_COUNTER == Training set size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Mimi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in subtract\n",
      "C:\\Users\\Mimi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in subtract\n",
      "C:\\Users\\Mimi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Mimi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\Mimi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_new, beta_new = M_step(D_reuters, k, phi_list, gamma_list, alpha, vocab_list)\n",
    "print(alpha_new)\n",
    "beta_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1578574395942,
     "user": {
      "displayName": "Peter Steiglechner",
      "photoUrl": "",
      "userId": "08437803724975041227"
     },
     "user_tz": -60
    },
    "id": "m3Mvnv9tKlTp",
    "outputId": "b94ced5a-5811-4e7c-f75a-5116ffdaadb1"
   },
   "outputs": [],
   "source": [
    "plt.plot(likelihood_list, 'x')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AdvML_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
