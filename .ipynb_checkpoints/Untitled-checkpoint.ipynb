{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD2434_Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS:\n",
    "\n",
    "k - number of topics\n",
    "N - number of words in a document (different for each document)\n",
    "M - number of documents in a corpus\n",
    "\n",
    "Model parameters:\n",
    "z_n - [k] dimension vector; topic distribution for word n \n",
    "Theta - [k] dimension vector; mixture weights\n",
    "alpha - [k] dimension vector; prior probability for theta (mixture weights) (alpha > 0)\n",
    "beta - [k x V] dimension matrix; beta_ij = p(w^j = 1 | z^i = 1) \n",
    "                                 probability for a specific word j given a specific topic i\n",
    "D - list of [V x N] dimension matrices, that is M long = [\\mathbf{w}_1, ... \\mathbf{w}_M];\n",
    "                                 where \\mathbf{w} = [w_1,...,w_N] is [V x N] (one document consisting of N words) \n",
    "\n",
    "\n",
    "Variational parameters:\n",
    "Gamma - [k] dimension vector; determines Theta in the Variational Model\n",
    "Phi = phi_1 .. phi_N - [N x k] dimension matrix; determines the probability distribution \n",
    "                                 for topics z of words in the Variational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "from scipy import special, misc\n",
    "from scipy.special import digamma, gammaln, polygamma\n",
    "import pandas as pd\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Document Corpus\n",
    "Download the data from \n",
    "https://github.com/Blei-Lab/lda-c/blob/master/example/ap.tgz\n",
    "\n",
    "We can directly load the file \"ap.dat\" which contains:\n",
    "\n",
    "1 line = 1 document,\n",
    "\n",
    "[number of different words in doc] [word index (where the one is in w_n]:[how often it occurs in the doc] [word index 2]:[occurences 2] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = np.genfromtxt('ap/vocab.txt',  dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_data(corpus_file, vocabulary_file, stopwords_file):\n",
    "    \"\"\"\n",
    "    Reads the corpus from the .txt file into a list of lists;\n",
    "    a list for each document which contains a list of all the \n",
    "    words as strings.\n",
    "    Input parameters:\n",
    "    corpus_file - path to the corpus file\n",
    "    vocabulary_file - path to the vocabulary file\n",
    "    stopwords_file - path to the stopwords file\n",
    "    \"\"\"\n",
    "    vocabulary = np.genfromtxt(vocabulary_file,  dtype='str')\n",
    "    special_chars = '1234567890~!@#Â£$%^&*()_+,./<>?\\|\"]}\\'[{`-'\n",
    "    corpus = []\n",
    "    \n",
    "    # read in stopwords from file into a list\n",
    "    stopwords = [] \n",
    "    with open(stopwords_file, 'r') as file:\n",
    "        stop_words = file.read().replace(',', ' ')\n",
    "        for word in stop_words.split():\n",
    "            stopwords.append(word) \n",
    "    \n",
    "    with open(corpus_file, 'r') as text:\n",
    "        doc = ''\n",
    "        new = False\n",
    "        for line in text:\n",
    "            if new: # reached a new document\n",
    "                if line.strip() != '</TEXT>': # until we reach the new doc\n",
    "                    for char in special_chars: # remove punctuation etc,\n",
    "                        line = line.replace(char, '') \n",
    "                    doc += line\n",
    "                else: # we've reached a new doc again\n",
    "                    doc = doc.lower() # all words lowercase\n",
    "                    words = np.array(doc.split())\n",
    "                    # PETER EDIT: next two lines\n",
    "                    doc = [word for word in words if (  (word not in stopwords) and (word in vocabulary)  )]\n",
    "                    corpus.append(doc)\n",
    "                    doc = ''\n",
    "            elif line.strip() == '<TEXT>': new = True\n",
    "\n",
    "    \n",
    "    return corpus, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocabulary = clean_up_data('ap/ap.txt', 'ap/vocab.txt', 'ap/stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(documents, vocabulary, k):\n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "    \n",
    "    # Initialize alpha \n",
    "    # alpha = np.ones([M,k]) * 50/k # for every document, for every topic\n",
    "    alpha = np.ones(k)*50/k\n",
    "    eta = 5/k\n",
    "    \n",
    "    Lambda = np.random.rand(k,V) * 0.5 + 0.5\n",
    "    \n",
    "    # Initialize beta\n",
    "    beta = np.zeros([k,V]) # for every topic, for every word in the vocabulary\n",
    "    for i in range(k):\n",
    "        beta[i] = np.random.uniform(0, 1, V)\n",
    "        beta[i] = beta[i] / np.sum(beta[i])\n",
    "    \n",
    "    # Initialize phi and gamma\n",
    "    phi = []\n",
    "    gamma = np.zeros([M,k]) # for every document, for every topic\n",
    "    for m in range(M):\n",
    "        doc = np.array(documents[m])\n",
    "        N = len(doc)\n",
    "        phi.append(np.ones([N,k]) * 1/float(k)) # uniform over topics\n",
    "        \n",
    "        for i in range(k):\n",
    "            gamma[m][i] = alpha[i] + N/float(k)\n",
    "        #m += 1 # WHYYYYYYY?\n",
    "        \n",
    "    return alpha, eta, beta, gamma, phi, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lower_bound_likelihood(phi, gamma, alpha, beta, document, vocabulary,k):\n",
    "    '''\n",
    "    This calculates the lower bound of L(gamma, phi, alpha, beta)\n",
    "    Ie. equation 15 in the paper in Appendix 3.\n",
    "    '''\n",
    "   \n",
    "    N, k = phi.shape\n",
    "    k, V = beta.shape\n",
    "    \n",
    "    loggamma_sum = lambda x: scipy.special.gammaln(np.sum(x))\n",
    "    loggamma_x_i = lambda x, i: np.log(scipy.special.gamma(x[i]))\n",
    "    E_log_thetai_givenGamma = lambda i:  (psi(gamma[i]) - psi(np.sum(gamma))) \n",
    "\n",
    "    term0 = loggamma_sum(alpha) - loggamma_sum(gamma)\n",
    "    term_kSum=0\n",
    "    for i in range(k):\n",
    "        E = E_log_thetai_givenGamma(i)\n",
    "        term_kSum += -loggamma_x_i(alpha,i) + (alpha[i]-1) * E\n",
    "        term_kSum += gammaln(gamma[i]) - (gamma[i] - 1) * E\n",
    "\n",
    "        term_knSum = 0\n",
    "        term_knvSum = 0\n",
    "        for n in range(N):\n",
    "            if phi[n,i] == 0:\n",
    "                print(\"Error: Phi[\",n,i,\"] == 0\")\n",
    "            term_knSum += phi[n,i] * E_log_thetai_givenGamma(i)\n",
    "            term_knSum += - phi[n,i] * np.log(phi[n,i])\n",
    "            \n",
    "            v = np.where(vocabulary == document[n])[0][0] # here w_n is not a vector\n",
    "            if beta[i,v] <= 0:\n",
    "                print(\"Error: beta[\"+i,v,\"]<=0\")\n",
    "            #L+= phi[n,i] * np.log(beta[i,v]) \n",
    "            term_knvSum += phi[n,i] * np.log(beta[i,v]) \n",
    "\n",
    "    #print(term0,term_knSum, term_kSum)\n",
    "    L_terms = term0 + term_knSum + term_kSum + term_knvSum\n",
    "    \n",
    "    return L_terms\n",
    "    \n",
    "\n",
    "\n",
    "def psi(gamma_i):\n",
    "    # this is the first derivative (via Taylor approximation) of the log \\Gamma function\n",
    "    # according to Wikipedia this is the \"digamma\" function\n",
    "    return scipy.special.digamma(gamma_i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeDocumentLikelihood(wd, alpha, eta, gamma, digamma_gamma, phi, digamma_lambda, d):\n",
    "\n",
    "    V, K = digamma_lambda.shape\n",
    "    N = len(wd)\n",
    "    \n",
    "    E_theta_alpha = gammaln(alpha*K) - K * gammaln(alpha) \\\n",
    "                        + (alpha-1) * np.sum(digamma_gamma)\n",
    "    \n",
    "    E_z_theta = np.dot(np.sum(phi[d][:N,:], axis = 0), digamma_gamma)\n",
    "    \n",
    "    E_w_z_beta = np.sum(digamma_lambda[d,:] * phi[d][:N,:])\n",
    "    \n",
    "    E_theta_gamma = gammaln(np.sum(gamma[d,:])) - np.sum(gammaln(gamma[d,:])) \\\n",
    "                    + np.dot(gamma[d,:] - 1, digamma_gamma)\n",
    "\n",
    "    E_z_phi = np.sum(phi[d][:N,:] * np.log(phi[d][:N,:]))\n",
    "    \n",
    "\n",
    "    Likelihood = E_theta_alpha + E_z_theta + E_w_z_beta - E_theta_gamma - E_z_phi\n",
    "    return(Likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihood_smoothed(documents, alpha, eta, gamma, phi, Lambda):\n",
    "    \n",
    "    K, V = Lambda.shape\n",
    "    D = len(documents)\n",
    "    \n",
    "    digamma_lambda = digamma(Lambda.T) - digamma(np.sum(Lambda, axis = 1))\n",
    "    likelihood = np.zeros([D,1])\n",
    "    \n",
    "    for d in range(D):\n",
    "        digamma_gamma = digamma(gamma[d,:]) - digamma(np.sum(gamma[d,:]))\n",
    "\n",
    "        N = len(documents[d])\n",
    "        E_theta_alpha = gammaln(alpha*K) - K * gammaln(alpha) \\\n",
    "                            + (alpha-1) * np.sum(digamma_gamma)\n",
    "        E_z_theta = np.dot(np.sum(phi[d][:N,:], axis = 0), digamma_gamma)\n",
    "        E_w_z_beta = np.sum(digamma_lambda[d,:] * phi[d][:N,:])\n",
    "        E_theta_gamma = gammaln(np.sum(gamma[d,:])) - np.sum(gammaln(gamma[d,:])) \\\n",
    "                        + np.dot(gamma[d,:] - 1, digamma_gamma)\n",
    "        E_z_phi = np.sum(phi[d][:N,:] * np.log(phi[d][:N,:]))\n",
    "        \n",
    "        likelihood = E_theta_alpha + E_z_theta + E_w_z_beta - E_theta_gamma - E_z_phi\n",
    "\n",
    "        E_beta_eta = K * (gammaln(eta * V) - V * gammaln(eta)) + (eta - 1) * np.sum(digamma_lambda)\n",
    "        E_beta_lambda = np.sum(gammaln(np.sum(Lambda, axis = 1)) - np.sum(gammaln(Lambda), axis = 1)[np.newaxis,:]) \\\n",
    "                        + np.sum((Lambda - 1) * digamma_lambda.T)\n",
    "    \n",
    "    likelihood = np.sum(likelihood) + E_beta_eta - E_beta_lambda\n",
    "    \n",
    "    return(likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def copied(Phi, gamma, alpha, Beta, document, vocabulary, k):\n",
    "#    likelihood = 0.0\n",
    "#    V = len(vocabulary)\n",
    "#    words = np.array(document)\n",
    "#    N = len(words)\n",
    "#    print(N,V,k)\n",
    "#    \n",
    "#    alpha_sum = 0.0\n",
    "#    phi_gamma_sum = 0.0\n",
    "#    phi_logbeta_sum = 0.0\n",
    "#    entropy_sum = 0.0\n",
    "#    gamma_sum = 0.0\n",
    "#    \n",
    "#    alpha_sum += gammaln(np.sum(alpha))  \n",
    "#    gamma_sum -= gammaln(np.sum(gamma)) \n",
    "#    \n",
    "#    for i in range(k):\n",
    "#        alpha_sum += -gammaln(alpha[i]) + \\\n",
    "#                (alpha[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma)))\n",
    "#        \n",
    "#        for n in range(N):\n",
    "#            if Phi[n,i] > 0:\n",
    "#                w_indicator = np.sum(np.in1d(vocabulary, words[n]))   \n",
    "#                phi_gamma_sum += Phi[n,i] * (digamma(gamma[i]) - digamma(np.sum(gamma[:])))\n",
    "#                entropy_sum += Phi[n,i] * np.log(Phi[n,i])\n",
    "#                for j in range(V):\n",
    "#                    if Beta[i,j] > 0:\n",
    "#                        phi_logbeta_sum += Phi[n,i] * w_indicator * np.log(Beta[i,j])\n",
    "#                        #phi_logbeta_sum+=0\n",
    "#        \n",
    "#        gamma_sum += gammaln(gamma[i]) - \\\n",
    "#                    (gamma[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma[:])))\n",
    "#        \n",
    "#        print(\"i: \")\n",
    "#        print(-gammaln(alpha[i]) + \\\n",
    "#                (alpha[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma))))\n",
    "#        print(gammaln(gamma[i]) - \\\n",
    "#                    (gamma[i] - 1) * (digamma(gamma[i]) - digamma(np.sum(gamma[:]))))\n",
    "#        \n",
    "#    \n",
    "#    likelihood += (alpha_sum + phi_gamma_sum + phi_logbeta_sum - gamma_sum - entropy_sum) \n",
    "#    \n",
    "#    print(alpha_sum, gamma_sum, phi_gamma_sum,entropy_sum)\n",
    "#    print(\"L=\",alpha_sum+phi_gamma_sum-gamma_sum-entropy_sum)\n",
    "#\n",
    "#    print(phi_logbeta_sum)\n",
    "#    return likelihood#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_phi_gamma(k, phi, gamma, alpha, beta, document, vocabulary, tol=1e-5, MAX_STEPS = 100):\n",
    "    \n",
    "    likelihood = 0.0\n",
    "    iterations = 0\n",
    "    converged = False\n",
    "    \n",
    "    words = np.array(document)\n",
    "    N = len(words)\n",
    "\n",
    "    while (not converged) and (iterations < MAX_STEPS):\n",
    "        iterations += 1\n",
    "            \n",
    "        phi_old = phi\n",
    "        phi = np.zeros([N,k])\n",
    "        gamma_old = gamma\n",
    "            \n",
    "        for n in range(N):\n",
    "            word = words[n]\n",
    "            if len(np.where(vocabulary == word)[0]) > 0: # word exists in vocabulary\n",
    "                for i in range(k):                \n",
    "                    beta_ = beta[i, np.where(vocabulary == word)]\n",
    "                    phi[n, i] = beta_[0][0] * np.exp(digamma(gamma[i]) - digamma(np.sum(gamma)))\n",
    "                phi[n,:] = phi[n,:] / np.sum(phi[n,:])   \n",
    "        gamma = alpha + np.sum(phi, axis=0)    \n",
    "            \n",
    "\n",
    "        # Convergence ctierion: did phi and gamma change significantly?\n",
    "        if (np.linalg.norm(phi - phi_old) < tol) and (np.linalg.norm(gamma - gamma_old) < tol):              \n",
    "            print(str(iterations) + ' iterations to converge.')\n",
    "                \n",
    "            likelihood += compute_lower_bound_likelihood(phi, gamma, alpha, beta, document, vocabulary, k)\n",
    "            converged = True\n",
    "    \n",
    "    return phi, gamma, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_phi_gamma_smoothed(documents, alpha, eta, gamma, phi, Lambda, tol = 1e-4, MAX_STEPS = 100):\n",
    "    K, V = Lambda.shape\n",
    "    D = len(documents)\n",
    "    \n",
    "    digamma_lambda = digamma(Lambda.T) - digamma(np.sum(Lambda, axis = 1))\n",
    "    for d in range(D):\n",
    "        likelihood = -1e9\n",
    "        converged = False\n",
    "        iterations = 0\n",
    "        \n",
    "        while(not converged):\n",
    "\n",
    "            iterations += 1\n",
    "            wd = documents[d]\n",
    "            \n",
    "            digamma_gamma = digamma(gamma[d,:]) - digamma(np.sum(gamma[d,:]))\n",
    "            \n",
    "            N = len(documents[d])\n",
    "            \n",
    "            phi[d][:N,:] = digamma_gamma + digamma_lambda[d,:]\n",
    "            phi[d][:N,:] = np.exp(phi[d][:N,:] - misc.logsumexp(phi[d][:N,:], axis = 1)[:,np.newaxis])\n",
    "            \n",
    "            gamma[d,:] = alpha + np.sum(phi[d][:N,:], axis = 0)\n",
    "            \n",
    "            newLikelihood = ComputeDocumentLikelihood(wd, alpha, eta, gamma, digamma_gamma, phi, digamma_lambda, d)\n",
    "            dlikelihood = abs((newLikelihood - likelihood)/likelihood)\n",
    "            likelihood = newLikelihood\n",
    "            \n",
    "            if(dlikelihood < tol).any():\n",
    "                converged = True\n",
    "    \n",
    "    #print gamma\n",
    "    return(phi, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lambda(phi, eta, documents, vocabulary, k):\n",
    "    \n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "    \n",
    "    Lambda = np.ones([k, V]) * eta\n",
    "    for m, doc in enumerate(documents):\n",
    "        words = np.array(doc)\n",
    "        phi_m = phi[m]\n",
    "        for i in range(k):\n",
    "            phi_ = phi_m[:,i]\n",
    "            for j in range(V):\n",
    "                word = vocabulary[j]\n",
    "                indicator = np.in1d(words, word)\n",
    "                indicator.astype(int)  \n",
    "                Lambda[i][j] += np.dot(indicator, phi_)\n",
    "                    \n",
    "    return Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_beta(phi, documents, vocabulary, k):\n",
    "    \n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "    \n",
    "    beta = np.zeros([k, V])\n",
    "    for m, doc in enumerate(documents):\n",
    "        words = np.array(doc)\n",
    "        phi_m = phi[m]\n",
    "        for i in range(k):\n",
    "            phi_ = phi_m[:,i]\n",
    "            for j in range(V):\n",
    "                word = vocabulary[j]\n",
    "                indicator = np.in1d(words, word)\n",
    "                indicator.astype(int) \n",
    "                beta[i][j] += np.dot(indicator, phi_)\n",
    "    beta = np.transpose(np.transpose(beta) / np.sum(beta, axis=1))\n",
    "\n",
    "    return beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha(alpha, gamma, k, M, max_iter=50, tol=1e-4):\n",
    "    \n",
    "    # Maria B version\n",
    "    temp = 0\n",
    "    for d in range(M):\n",
    "        temp_1 = np.sum(special.polygamma(0, gamma[d])) - np.sum(special.polygamma(0, np.sum(gamma, axis=1)))\n",
    "    \n",
    "    gradient = M * (k * special.polygamma(1, alpha) - special.polygamma(1, k*alpha))\n",
    "    gradient = gradient + temp\n",
    "\n",
    "    hessian = M * k * (k * special.polygamma(2, k*alpha) - special.polygamma(2, alpha))\n",
    "\n",
    "    temp = gradient / (hessian * alpha + gradient + tol)\n",
    "    if (alpha == 0).any():\n",
    "        alpha += 0.005\n",
    "\n",
    "    log_alpha = np.log(alpha) - temp\n",
    "    alpha = np.exp(log_alpha)    \n",
    "        \n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_eta(eta, gamma, k, V, M, max_iter=50, tol=1e-4):\n",
    "    \n",
    "\n",
    "    temp = 0\n",
    "    for d in range(M):\n",
    "        temp_1 = np.sum(special.polygamma(0, gamma[d])) - np.sum(special.polygamma(0, np.sum(gamma, axis=1)))\n",
    "    \n",
    "    gradient = V * (k * special.polygamma(1, eta) - special.polygamma(1, k*eta))\n",
    "    gradient = gradient + temp\n",
    "\n",
    "    hessian = V * k * (k * special.polygamma(2, k*eta) - special.polygamma(2, eta))\n",
    "\n",
    "    temp = gradient / (hessian * eta + gradient + tol)\n",
    "    if (eta == 0):\n",
    "        eta += 0.005\n",
    "\n",
    "    log_eta = np.log(eta) - temp\n",
    "    eta = np.exp(log_eta)    \n",
    "        \n",
    "    return eta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(phi, gamma, alpha, beta, documents, vocabulary, k):\n",
    "\n",
    "    for d, doc in enumerate(documents):\n",
    "        phi[d], gamma[d], likelihood = update_phi_gamma(k, phi[d], gamma[d], alpha, beta, doc, vocabulary)\n",
    "                \n",
    "    return phi, gamma, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step_smoothed(documents, alpha, eta, gamma, phi, Lambda, Tol = 1e-5, MAX_STEPS = 100):\n",
    "\n",
    "    K, V = Lambda.shape\n",
    "    D = len(documents)\n",
    "    \n",
    "    iterations = 0\n",
    "    likelihood = -1e9\n",
    "    converged = False\n",
    "    while(not(converged)):\n",
    "        \n",
    "        iterations += 1\n",
    "        phi, gamma = update_phi_gamma_smoothed(documents, alpha, eta, gamma, phi, Lambda)\n",
    "        Lambda = update_lambda(phi, eta, documents, vocabulary, k)\n",
    "\n",
    "\n",
    "        newLikelihood = compute_likelihood_smoothed(documents, alpha, eta, gamma, phi, Lambda)\n",
    "        dlikelihood = abs((newLikelihood - likelihood)/likelihood)\n",
    "        likelihood = newLikelihood\n",
    "\n",
    "        if(dlikelihood < Tol).any():\n",
    "            print('E-step converged after %d iterations' %iterations)\n",
    "            converged = True\n",
    "    return(phi, gamma, Lambda, likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(phi, gamma, alpha, documents, vocabulary, k):\n",
    "    print('M-step')\n",
    "    beta = update_beta(phi, documents, vocabulary, k)\n",
    "    alpha = update_alpha(alpha, gamma, k, M)\n",
    "    \n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step_smoothed(phi, gamma, alpha, eta, documents, vocabulary, k):\n",
    "    print('M-step')\n",
    "    \n",
    "    M = len(documents)\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    alpha = update_alpha(alpha, gamma, k, M)\n",
    "    eta = update_eta(eta, gamma, k, V, M)\n",
    "    \n",
    "    return alpha, eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_EM(phi_init, gamma_init, alpha_init, beta_init, documents, vocabulary, k, tol=1e-5):\n",
    "    print('Variational EM')\n",
    "    \n",
    "    M = len(documents)\n",
    "    \n",
    "    likelihood = 0\n",
    "    likelihood_old = 0.000004\n",
    "    \n",
    "    iteration = 1 # Initialization step is the first step\n",
    "    \n",
    "    phi = phi_init\n",
    "    gamma = gamma_init\n",
    "    alpha = alpha_init\n",
    "    beta = beta_init\n",
    "    \n",
    "    converged = False\n",
    "    \n",
    "    while (not converged):\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Update parameters \n",
    "        if likelihood == 0:\n",
    "            print(\"Likelihood==0\")\n",
    "            likelihood_old = 0.005\n",
    "        else:\n",
    "            likelihood_old = likelihood\n",
    "        phi_old = phi \n",
    "        gamma_old = gamma \n",
    "        alpha_old = alpha\n",
    "        beta_old = beta\n",
    "    \n",
    "        phi, gamma, likelihood = \\\n",
    "            E_step(phi_old, gamma_old, alpha_old, beta_old, documents, vocabulary, k)\n",
    "        alpha, Beta = \\\n",
    "            M_step(phi, gamma, alpha_old, documents, vocabulary, k)\n",
    "                \n",
    "        if iteration > 15:\n",
    "            break\n",
    "        \n",
    "        # check convergence\n",
    "        if (np.abs((likelihood-likelihood_old)/likelihood_old) > tol):\n",
    "            if (iteration > 2):\n",
    "                converged = True\n",
    "        \n",
    "    return phi, gamma, alpha, Beta, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_EM_smoothed(phi_init, gamma_init, alpha_init, beta_init, Lambda_init, eta_init, documents, vocabulary, k, tol=1e-5):\n",
    "    print('Variational EM')\n",
    "    \n",
    "    M = len(documents)\n",
    "    \n",
    "    likelihood = 0\n",
    "    likelihood_old = 0.000004\n",
    "    \n",
    "    iteration = 1 # Initialization step is the first step\n",
    "    \n",
    "    phi = phi_init\n",
    "    gamma = gamma_init\n",
    "    alpha = alpha_init\n",
    "    beta = beta_init\n",
    "    Lambda = Lambda_init\n",
    "    eta = eta_init\n",
    "    \n",
    "    converged = False\n",
    "    \n",
    "    while (not converged):\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Update parameters \n",
    "        if likelihood == 0:\n",
    "            print(\"Likelihood==0\")\n",
    "            likelihood_old = 0.005\n",
    "        else:\n",
    "            likelihood_old = likelihood\n",
    "        phi_old = phi \n",
    "        gamma_old = gamma \n",
    "        alpha_old = alpha\n",
    "        beta_old = beta\n",
    "        Lambda_old = Lambda\n",
    "        eta_old = eta\n",
    "        \n",
    "    \n",
    "        phi, gamma, Lambda, likelihood = \\\n",
    "            E_step_smoothed(documents, alpha_old, eta_old, gamma_old, phi_old, Lambda_old)\n",
    "        alpha, eta = \\\n",
    "            M_step_smoothed(phi, gamma, alpha, eta, documents, vocabulary, k)\n",
    "                \n",
    "        if iteration > 15:\n",
    "            break\n",
    "        \n",
    "        # check convergence\n",
    "        if (np.abs((likelihood-likelihood_old)/likelihood_old) > tol):\n",
    "            if (iteration > 2):\n",
    "                converged = True\n",
    "        \n",
    "    return phi, gamma, Lambda, alpha, eta, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN: LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "corpus_reduced = corpus[:3]\n",
    "M = len(corpus_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_init, eta_init, beta_init, gamma_init, phi_init, Lambda_init = initialize_parameters(corpus_reduced, vocabulary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational EM\n",
      "Likelihood==0\n",
      "38 iterations to converge.\n",
      "42 iterations to converge.\n",
      "39 iterations to converge.\n",
      "M-step\n",
      "44 iterations to converge.\n",
      "48 iterations to converge.\n",
      "43 iterations to converge.\n",
      "M-step\n"
     ]
    }
   ],
   "source": [
    "phi, gamma, alpha, beta, likelihood = \\\n",
    "        variational_EM(phi_init, gamma_init, alpha_init, beta_init, corpus_reduced, vocabulary, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN: smoothed LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_init, eta_init, beta_init, gamma_init, phi_init, Lambda_init = initialize_parameters(corpus_reduced, vocabulary, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational EM\n",
      "Likelihood==0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bjeli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: `logsumexp` is deprecated!\n",
      "Importing `logsumexp` from scipy.misc is deprecated in scipy 1.0.0. Use `scipy.special.logsumexp` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-step converged after 10 iterations\n",
      "M-step\n",
      "NEW DEBU\n",
      "1.0352422730586692\n",
      "E-step converged after 2 iterations\n",
      "M-step\n",
      "NEW DEBU\n",
      "0.6663281132982769\n"
     ]
    }
   ],
   "source": [
    "phi, gamma, Lambda, alpha, eta, likelihood = \\\n",
    "variational_EM_smoothed(phi_init, gamma_init, alpha_init, beta_init, Lambda_init, eta_init, corpus_reduced, vocabulary, k, tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.03524227, 1.03524227, 1.36852775, ..., 1.03524227, 1.03524227,\n",
       "        1.03524227],\n",
       "       [1.03524227, 1.03524227, 1.36835547, ..., 1.03524227, 1.03524227,\n",
       "        1.03524227],\n",
       "       [1.03524227, 1.03524227, 1.3688436 , ..., 1.03524227, 1.03524227,\n",
       "        1.03524227]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1b76d784438>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(gamma.T)\n",
    "plt.xlabel(\"Documents\")\n",
    "plt.ylabel(\"topic 1..k\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.pcolormesh(Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shot', 'bloomberg', 'peres', 'liberace', 'police']\n",
      "['offer', 'mrs', 'teacher', 'years', 'school']\n",
      "['rappaport', 'mrs', 'peres', 'bechtel', 'official']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for l in range(k):\n",
    "    beta_topic = beta[l,:]\n",
    "    beta_topic_top4 = np.argsort(beta_topic)[-5:]\n",
    "    plt.plot(beta_topic)\n",
    "    print([w for w in np.array(vocabulary)[beta_topic_top4][:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITE TEXT WITH COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For three years Charles S Robb was out of the spotlight that had become so familiar first as the soninlaw of President Lyndon Johnson and then as Democratic governor of this conservative state But on Tuesday the yearold lawyer reentered the national arena in decisive style fashioning a huge victory over Republican longshot Maurice Dawkins a retired black minister and Washington lobbyist Robb said today he won because we attempted to identify with mainstream values that are crucial to success at the national level such as strong defense and fiscal responsibility With  percent of the precincts counted Robb had'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_full_text(corpus_file, number_of_text):\n",
    "    fulltext_words=[]\n",
    "    fulltext_allwords=[]\n",
    "    #number_of_text=2\n",
    "    text_counter=0\n",
    "    special_chars = '1234567890~!@#Â£$%^&*()_+,./<>?\\|\"]}\\'[{`-'\n",
    "\n",
    "    with open(corpus_file, 'r') as text:\n",
    "        new=False\n",
    "        for line in text:\n",
    "            if new:\n",
    "                #print(line.strip()[0], line.strip()[:10])\n",
    "                if line.strip()[0]==\"<\":\n",
    "                    pass\n",
    "                else:\n",
    "                    #print(\"FOUND\", text)\n",
    "                    text_counter+=1\n",
    "                    if text_counter==number_of_text:\n",
    "                        #print(\" CORRECT\")\n",
    "                        new_text=line\n",
    "                        fulltext=new_text\n",
    "                        words = np.array(new_text.split())\n",
    "                        for word in words:\n",
    "                            fulltext_allwords.append(word)\n",
    "                            for char in special_chars: # remove punctuation etc,\n",
    "                                word = word.replace(char, '') \n",
    "                            fulltext_words.append(word)\n",
    "\n",
    "\n",
    "            else:\n",
    "                if line.strip() == \"<TEXT>\":\n",
    "                    new=True\n",
    "    return fulltext_words, fulltext_allwords\n",
    "\n",
    "fulltext_words, fulltext_allwords = get_full_text('ap/ap.txt', 13)\n",
    "\" \".join(fulltext_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "years= 1, years= 1, years= 1, "
     ]
    }
   ],
   "source": [
    "colors=['blue','green', 'red']\n",
    "fulltext_colors=[]\n",
    "\n",
    "how_significant=2.5\n",
    "significance=[]\n",
    "for word in fulltext_words:\n",
    "    if word in vocabulary:\n",
    "        v = np.where(vocabulary==word)[0][0]\n",
    "        #print(v,word_beta )\n",
    "        word_beta = beta[:,v]\n",
    "        #significance.append(np.max(word_beta)/np.mean(word_beta))\n",
    "        if np.max(word_beta)>np.mean(beta):\n",
    "            if np.max(word_beta)> how_significant*np.mean(word_beta):\n",
    "                #significance =  (np.max(word_beta) / np.mean(word_beta) > 10)\n",
    "                topic = np.where(np.max(word_beta)==word_beta)[0][0]\n",
    "                color = colors[topic]\n",
    "                print(word+\"=\",str( topic)+\", \", end=\"\")\n",
    "\n",
    "            else:\n",
    "                color='k'\n",
    "        else:\n",
    "            color='k'\n",
    "    else: \n",
    "        color='k'\n",
    "    fulltext_colors.append(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mFor \u001b[0mthree \u001b[0;32;48myears, \u001b[0mCharles \u001b[0mS. \u001b[0mRobb \u001b[0mwas \u001b[0mout \u001b[0mof \u001b[0mthe \u001b[0mspotlight \u001b[0mthat \u001b[0mhad \u001b[0mbecome \u001b[0mso \u001b[0mfamiliar, \u001b[0mfirst \u001b[0mas \u001b[0mthe \u001b[0mson-in-law \u001b[0mof \u001b[0mPresident \u001b[0mLyndon \u001b[0mJohnson \u001b[0mand \u001b[0mthen \u001b[0mas \u001b[0mDemocratic \u001b[0mgovernor \u001b[0mof \u001b[0mthis \u001b[0mconservative \u001b[0mstate. \u001b[0mBut \u001b[0mon \u001b[0mTuesday, \u001b[0mthe \u001b[0m49-year-old \u001b[0mlawyer \u001b[0mre-entered \u001b[0mthe \u001b[0mnational \u001b[0marena \u001b[0min \u001b[0mdecisive \u001b[0mstyle, \u001b[0mfashioning \u001b[0ma \u001b[0mhuge \u001b[0mvictory \u001b[0mover \u001b[0mRepublican \u001b[0mlong-shot \u001b[0mMaurice \u001b[0mDawkins, \u001b[0ma \u001b[0mretired \u001b[0mblack \u001b[0mminister \u001b[0mand \u001b[0mWashington \u001b[0mlobbyist. \u001b[0mRobb \u001b[0msaid \u001b[0mtoday \u001b[0mhe \u001b[0mwon \u001b[0mbecause \u001b[0m``we \u001b[0mattempted \u001b[0mto \u001b[0midentify \u001b[0mwith \u001b[0mmainstream \u001b[0mvalues \u001b[0mthat \u001b[0mare \u001b[0mcrucial \u001b[0mto \u001b[0msuccess \u001b[0mat \u001b[0mthe \u001b[0mnational \u001b[0mlevel,'' \u001b[0msuch \u001b[0mas \u001b[0mstrong \u001b[0mdefense \u001b[0mand \u001b[0mfiscal \u001b[0mresponsibility. \u001b[0mWith \u001b[0m99 \u001b[0mpercent \u001b[0mof \u001b[0mthe \u001b[0mprecincts \u001b[0mcounted, \u001b[0mRobb \u001b[0mhad \u001b[0m1,448,389 \u001b[0mvotes \u001b[0mor \u001b[0m71 \u001b[0mpercent, \u001b[0mto \u001b[0mDawkins' \u001b[0m587,887 \u001b[0mvotes \u001b[0mor \u001b[0m29 \u001b[0mpercent. \u001b[0mThe \u001b[0mformer \u001b[0mMarine \u001b[0mcombat \u001b[0mofficer \u001b[0mhas \u001b[0mbuilt \u001b[0ma \u001b[0mcareer \u001b[0mby \u001b[0mmaking \u001b[0mDemocrats \u001b[0melectable \u001b[0min \u001b[0mconservative \u001b[0mVirginia. \u001b[0mOnce \u001b[0mknown \u001b[0monly \u001b[0mas \u001b[0mthe \u001b[0mformer \u001b[0mWhite \u001b[0mHouse \u001b[0mmilitary \u001b[0msocial \u001b[0maide \u001b[0mwho \u001b[0mmarried \u001b[0mLBJ's \u001b[0mdaughter, \u001b[0mLynda \u001b[0mBird \u001b[0mJohnson, \u001b[0mhe \u001b[0mwon \u001b[0mthe \u001b[0mlieutenant \u001b[0mgovernor's \u001b[0mrace \u001b[0min \u001b[0m1977 \u001b[0min \u001b[0mhis \u001b[0mfirst \u001b[0mbid \u001b[0mfor \u001b[0melective \u001b[0moffice. \u001b[0mFour \u001b[0;32;48myears \u001b[0mlater, \u001b[0mhe \u001b[0mran \u001b[0mfor \u001b[0mgovernor \u001b[0min \u001b[0mthe \u001b[0mfirst \u001b[0msweep \u001b[0mby \u001b[0mDemocrats \u001b[0mof \u001b[0mthe \u001b[0mstate's \u001b[0mtop \u001b[0mthree \u001b[0moffices \u001b[0msince \u001b[0m1965. \u001b[0mRobb \u001b[0mwas \u001b[0ma \u001b[0mpopular \u001b[0mgovernor \u001b[0mwho \u001b[0mwas \u001b[0mcredited \u001b[0mwith \u001b[0moverhauling \u001b[0mthe \u001b[0mstate \u001b[0mbureaucracy \u001b[0mand \u001b[0mmaking \u001b[0mmajor \u001b[0mgains \u001b[0min \u001b[0meducation \u001b[0mfunding. \u001b[0mHe \u001b[0malso \u001b[0mopened \u001b[0mpositions \u001b[0mof \u001b[0mauthority \u001b[0min \u001b[0mstate \u001b[0mgovernment \u001b[0mto \u001b[0mblacks \u001b[0mand \u001b[0mwomen \u001b[0mand \u001b[0mappointed \u001b[0mVirginia's \u001b[0mfirst \u001b[0mblack \u001b[0mSupreme \u001b[0mCourt \u001b[0mmember. \u001b[0mThe \u001b[0mformer \u001b[0mgovernor \u001b[0mwas \u001b[0malso \u001b[0mone \u001b[0mof \u001b[0mthe \u001b[0marchitects \u001b[0mof \u001b[0mlast \u001b[0mspring's \u001b[0mSuper \u001b[0mTuesday \u001b[0mpresidential \u001b[0mprimary, \u001b[0mintended \u001b[0min \u001b[0mpart \u001b[0mto \u001b[0mgive \u001b[0mthe \u001b[0mSouthern \u001b[0mvote \u001b[0mcollective \u001b[0mstrength. \u001b[0mBut \u001b[0mRobb's \u001b[0mtenure \u001b[0mwas \u001b[0mshaken \u001b[0mby \u001b[0mprison \u001b[0mtroubles \u001b[0mthat \u001b[0mdrew \u001b[0mnational \u001b[0mattention \u001b[0mwhen \u001b[0msix \u001b[0mdeath \u001b[0mrow \u001b[0minmates \u001b[0mescaped \u001b[0min \u001b[0mMay \u001b[0m1984. \u001b[0mRobb, \u001b[0mwho \u001b[0mcould \u001b[0mnot \u001b[0msucceed \u001b[0mhimself \u001b[0munder \u001b[0mVirginia's \u001b[0mconstitution, \u001b[0mhad \u001b[0mbeen \u001b[0mout \u001b[0mof \u001b[0moffice \u001b[0mfor \u001b[0mthree \u001b[0;32;48myears \u001b[0mand \u001b[0mpracticing \u001b[0mlaw \u001b[0muntil \u001b[0mhis \u001b[0mbid \u001b[0mfor \u001b[0mthe \u001b[0mSenate. \u001b[0m``I've \u001b[0mbeen \u001b[0munemployed \u001b[0mfor \u001b[0ma \u001b[0mlong \u001b[0mtime, \u001b[0mand \u001b[0mit \u001b[0mlooks \u001b[0mlike \u001b[0mI \u001b[0mjust \u001b[0mgot \u001b[0ma \u001b[0mjob,'' \u001b[0mhe \u001b[0msaid. "
     ]
    }
   ],
   "source": [
    "#http://ozzmaker.com/add-colour-to-text-in-python/\n",
    "#print(\"Examples of how to use ANSI COLOR: \\033[1;37;40m White          \\033[0m 1;37;40m            \\033[0;37;40m Light Grey \\033[0m 0;37;40m               \\033[0;37;48m Black      \\033[0m 0;37;48m\")\n",
    "colors_ansi=[34, 32,31] #blue, green, red\n",
    "             \n",
    "             \n",
    "for a in range(len(fulltext_allwords)):\n",
    "    if fulltext_colors[a]=='k':\n",
    "        #IF WE WANT TO FOCUS ON THE TOPIC WORDS:\n",
    "        #print(\"\\033[0;37;48m\"+fulltext_allwords[a], end=\" \")\n",
    "        print(\"\\033[0m\"+fulltext_allwords[a], end=\" \")\n",
    "        \n",
    "        #print(fulltext_allwords[a], end=\" \")\n",
    "    else:\n",
    "        for j in range(k):\n",
    "             if fulltext_colors[a]==colors[j]:\n",
    "                print(\"\\033[0;\"+str(colors_ansi[j])+\";48m\"+fulltext_allwords[a], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mTOPIC: 0: \u001b[0;34;48m\n",
      "\u001b[0m\n",
      "\u001b[0mTOPIC: 1: \u001b[0;32;48myears, years, years\n",
      "\u001b[0m\n",
      "\u001b[0mTOPIC: 2: \u001b[0;31;48m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "topic_words =[]\n",
    "for i in range(k):\n",
    "    topic_words.append(np.where(np.array(fulltext_colors)==colors[i]))\n",
    "    print(\"\\033[0mTOPIC: \"+str(i)+\": \"+\"\\033[0;\"+str(colors_ansi[i])+\";48m\"+ \", \".join(np.array(fulltext_words)[topic_words[i]]))\n",
    "    print(\"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REUTERS DATA (get from the external program to keep this a bit tidier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\bjeli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_max_reuters =  7769\n",
      "We have  100  Documents with sizes:  [633, 259, 119, 155, 115, 34, 250, 83, 182, 133, 126, 85, 84, 111, 99, 101, 67, 26, 73, 140, 108, 173, 49, 236, 109, 111, 365, 42, 66, 49, 130, 90, 52, 21, 136, 14, 28, 102, 110, 67, 72, 143, 196, 31, 24, 34, 601, 32, 123, 958, 193, 118, 36, 55, 54, 113, 36, 67, 569, 25, 100, 185, 60, 145, 78, 61, 10, 87, 252, 61, 151, 466, 128, 44, 25, 179, 180, 54, 167, 42, 76, 114, 301, 506, 78, 83, 51, 111, 254, 10, 136, 186, 127, 145, 85, 34, 34, 91, 68, 77]\n"
     ]
    }
   ],
   "source": [
    "from getReuters import D_reuters as corpus_reuters\n",
    "from getReuters import vocab_list as vocabulary_reuters\n",
    "vocabulary_reuters=np.array(vocabulary_reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_reuters = 2\n",
    "corpus_reuters_reduced = corpus_reuters[:10]\n",
    "M_reuters = len(corpus_reuters_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-92813c4c58a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0malpha_init_reuters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_init_reuters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_init_reuters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_init_reuters\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0minitialize_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_reuters_reduced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary_reuters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_reuters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "alpha_init_reuters, beta_init_reuters, gamma_init_reuters, phi_init_reuters =\\\n",
    "    initialize_parameters(corpus_reuters_reduced, vocabulary_reuters, k_reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_reuters, gamma_reuters, alpha_reuters, Beta_reuters, likelihood_reuters = \\\n",
    "        variational_EM(phi_init_reuters, gamma_init_reuters, alpha_init_reuters, beta_init_reuters, \n",
    "                       corpus_reuters_reduced, vocabulary_reuters, k_reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Analysis for REUTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(Beta_reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_LDA_vEM():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
